{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evH6sjsxsKWE"
   },
   "source": [
    "# A pendulum video and autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "trlxwPDloKs5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import sys\n",
    "syspath = 'SindyPendulum/'\n",
    "if syspath not in sys.path:\n",
    "    sys.path.append(syspath)\n",
    "    \n",
    "from sindy_library import SINDyLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qNbEXw0Uo3zR",
    "outputId": "7eabf512-f2cf-4429-bc0e-9af2374ed4cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VneYLUfl5bz"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pend(y, t,):\n",
    "    theta, omega = y\n",
    "    dydt = [omega, - np.sin(theta)]\n",
    "    return dydt\n",
    "\n",
    "def pend_damp(y, t,):\n",
    "    theta, omega = y\n",
    "    dydt = [omega, - np.sin(theta)-0.1*omega]\n",
    "    return dydt\n",
    "\n",
    "def select_ics(theta0,omegaics0):\n",
    "    ics = []\n",
    "    for i in range(n_ics):\n",
    "        for j in range(n_ics):\n",
    "            lim = (np.abs((omega0[j]**2)/2 - np.cos(theta0[i])))\n",
    "            if lim <  0.99 :\n",
    "                ics.append((theta0[i],omega0[j]))\n",
    "    return ics\n",
    "\n",
    "def wrap_to_pi(z):\n",
    "    z_mod = z % (2*np.pi)\n",
    "    subtract_m = (z_mod > np.pi) * (-2*np.pi)\n",
    "    return z_mod + subtract_m\n",
    "\n",
    "def image_gen(ics):\n",
    "    x = np.linspace(-1.5, 1.5, NX)\n",
    "    y = np.linspace(-1.5, 1.5, NY)\n",
    "    xx,yy = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "    data = np.empty([len(ics), len(t), len(x), len(y)],dtype = np.float32)\n",
    "    data2 = np.empty([len(ics), len(t), len(x), len(y)],dtype = np.float32)\n",
    "\n",
    "\n",
    "    for idx in range(len(ics)):\n",
    "        if(idx%100==0): print(idx,' su ', len(ics))\n",
    "        y0 = [ics[idx][0], ics[idx][1]]\n",
    "        sol = odeint(pend, y0,t)\n",
    "        theta = sol[:,0]\n",
    "        omega = sol[:,1]\n",
    "\n",
    "        temp = []\n",
    "        for i in range(len(theta)):\n",
    "            z = np.exp(- 20 *((xx - np.cos(theta[i] + np.pi/2))*(xx - \n",
    "                np.cos(theta[i] +np.pi/2))) - 20 * ((yy -np.sin(theta[i]+np.pi/2))*(yy -np.sin(theta[i]+np.pi/2))))\n",
    "            z = ((z - np.min(z))/(np.max(z)-np.min(z)))\n",
    "    \n",
    "            temp.append(z)\n",
    "        data[idx] = np.array(temp)\n",
    "        \n",
    "        temp = []\n",
    "        for i in range(len(omega)):            \n",
    "            exp = -20 * 2 * omega[i]*(np.cos(theta[i]+ np.pi/2) - np.sin(theta[i] + np.pi/2))\n",
    "            z = np.exp(- 20 *((xx - np.cos(theta[i] + np.pi/2))*(xx - \n",
    "                np.cos(theta[i] +np.pi/2))) - 20 * ((yy -np.sin(theta[i]+np.pi/2))*(yy -np.sin(theta[i]+np.pi/2))))\n",
    "            z = z * exp\n",
    "            z = ((z - np.min(z))/(np.max(z)-np.min(z)))\n",
    "            temp.append(z)\n",
    "        data2[idx] = np.array(temp)\n",
    "        \n",
    "    return data,data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  su  3724\n",
      "100  su  3724\n",
      "200  su  3724\n",
      "300  su  3724\n",
      "400  su  3724\n",
      "500  su  3724\n",
      "600  su  3724\n",
      "700  su  3724\n",
      "800  su  3724\n",
      "900  su  3724\n",
      "1000  su  3724\n",
      "1100  su  3724\n",
      "1200  su  3724\n",
      "1300  su  3724\n",
      "1400  su  3724\n",
      "1500  su  3724\n",
      "1600  su  3724\n",
      "1700  su  3724\n",
      "1800  su  3724\n",
      "1900  su  3724\n",
      "2000  su  3724\n",
      "2100  su  3724\n",
      "2200  su  3724\n",
      "2300  su  3724\n",
      "2400  su  3724\n",
      "2500  su  3724\n",
      "2600  su  3724\n",
      "2700  su  3724\n",
      "2800  su  3724\n",
      "2900  su  3724\n",
      "3000  su  3724\n",
      "3100  su  3724\n",
      "3200  su  3724\n",
      "3300  su  3724\n",
      "3400  su  3724\n",
      "3500  su  3724\n",
      "3600  su  3724\n",
      "3700  su  3724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(186200, 2601)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ics = 80\n",
    "    \n",
    "#COSTANTI e PARAMETRI\n",
    "ta = 0.\n",
    "tb = 5.\n",
    "dt = 0.1\n",
    "# tb = 10\n",
    "# dt = 0.1\n",
    "NX = 51\n",
    "NY = 51\n",
    "\n",
    "\n",
    "t = np.arange(ta, tb ,dt)\n",
    "theta0 = np.linspace(-np.pi,np.pi,n_ics)\n",
    "omega0 = np.linspace(-2.1, 2.1,n_ics)\n",
    "\n",
    "ics = select_ics(theta0,omega0)\n",
    "data,data2 = image_gen(ics)\n",
    "\n",
    "\n",
    "#questo reshape serve per mandare al autoencoder delle immagini flat\n",
    "#TODO verifica che sia corretto questo rehsape --> dovrebb essere ok fatto prova su colab\n",
    "X = data.reshape((len(ics) * len(t),NX * NY))\n",
    "Xdot = data2.reshape((len(ics) * len(t),NX * NY))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f20ea563e48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[123].reshape((51,51)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f20ea3d60b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xdot[123].reshape((51,51)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdot[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UW2N4TZTbYsO",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([186200, 2601]), torch.Size([186200, 2601]), torch.float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.from_numpy(X).float().to(device)\n",
    "Xdot = torch.from_numpy(Xdot).float().to(device)\n",
    "X.shape, Xdot.shape, X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167580, 18620)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = round(X.shape[0] * 0.1)\n",
    "train_size = X.shape[0] - val_size\n",
    "train_size, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B2vYJ447_sxQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8192\n",
    "my_dataset = TensorDataset(X,Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_subset, val_subset = torch.utils.data.random_split(my_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, shuffle=True, batch_size=batch_size)\n",
    "val_loader   = DataLoader(val_subset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lRG3Ea4CWWl"
   },
   "source": [
    "# Network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5ZSdKOTO8dFN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size,latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,latent_dim)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            print(m)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))   \n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size,latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,128)\n",
    "        self.fc4 = nn.Linear(128,input_size)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))   \n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_size,latent_dim)\n",
    "        self.decoder = Decoder(input_size,latent_dim)\n",
    "        self.SINDyLibrary = SINDyLibrary(\n",
    "            device=device,\n",
    "            latent_dim=latent_dim,\n",
    "            include_biases=True,\n",
    "            include_states=True,\n",
    "            include_sin=True,\n",
    "            include_cos=True,\n",
    "            include_multiply_pairs=False, #non ho capito cosa é ma conta due volte le coppie  \n",
    "            poly_order=2,\n",
    "            include_sqrt=False,\n",
    "            include_inverse=False,\n",
    "            include_sign_sqrt_of_diff=False)\n",
    "        \n",
    "\n",
    "        self.XI = nn.Parameter(torch.full((self.SINDyLibrary.number_candidate_functions,latent_dim),1.,dtype = torch.float32,requires_grad=True,device = device))\n",
    "\n",
    "        self.XI_coefficient_mask = torch.ones((self.SINDyLibrary.number_candidate_functions,latent_dim),dtype = torch.float32, device=device)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        learning_rate = 1e-4\n",
    "        #return torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def t_derivative(self,input, xdot, weights, biases, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Compute the first order time derivatives by propagating through the network.\n",
    "        da[l]dt = xdot * da[l]dx = xdot * product(g'(w[l]a[l-1] + b[l])* w[l])\n",
    "        Arguments:\n",
    "            input - 2D tensorflow array, input to the network. Dimensions are number of time points\n",
    "            by number of state variables.\n",
    "            xdot - First order time derivatives of the input to the network. quello che conosciamo\n",
    "            weights - List of tensorflow arrays containing the network weights\n",
    "            biases - List of tensorflow arrays containing the network biases\n",
    "            activation - String specifying which activation function to use. Options are\n",
    "            'elu' (exponential linear unit), 'relu' (rectified linear unit), 'sigmoid',\n",
    "            or linear.\n",
    "\n",
    "        Returns:\n",
    "            dadt - Tensorflow array, first order time derivatives of the network output.\n",
    "        \"\"\"\n",
    "        a   = input\n",
    "        dadt = xdot #per le condizioni iniziali\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            for i in range(len(weights) - 1):\n",
    "                z = torch.matmul(a, weights[i].T) + biases[i]\n",
    "                a = torch.sigmoid(z)\n",
    "                gprime = a * (1-a)\n",
    "                dadt = gprime * torch.matmul(dadt, weights[i].T)\n",
    "            dadt = torch.matmul(dadt, weights[-1].T) #fuori dal ciclo bisogna ancora moltiplicare per i pesi dell ultimo livello\n",
    "            \n",
    "        elif activation == 'relu':\n",
    "            for i in range(len(weights) - 1):\n",
    "                z = torch.matmul(a, weights[i].T) + biases[i]\n",
    "                a = torch.relu(z)\n",
    "                dadt = (z > 0).float() * torch.matmul(dadt, weights[i].T)    \n",
    "            dadt = torch.matmul(dadt, weights[-1].T) #fuori dal ciclo bisogna ancora moltiplicare per i pesi dell ultimo livello\n",
    "        return dadt #nel caso che ci serve dadt sará l output dell encoder ossia le latent variables!\n",
    "\n",
    "    \n",
    "    \n",
    "    def compute_quantities(self,x,xdot):\n",
    "    \n",
    "        z = self.encoder(x)\n",
    "        xtilde = self.decoder(z)\n",
    "\n",
    "        theta = self.SINDyLibrary.transform(z) \n",
    "        zdot_hat = torch.matmul(theta, self.XI_coefficient_mask * self.XI)\n",
    "        \n",
    "        encoder_parameters = list(self.encoder.parameters())\n",
    "        encoder_weight_list = [w for w in encoder_parameters if len(w.shape) == 2]\n",
    "        encoder_biases_list = [b for b in encoder_parameters if len(b.shape) == 1]\n",
    "        zdot = self.t_derivative(x, xdot, encoder_weight_list, encoder_biases_list, activation='sigmoid')                                               \n",
    "\n",
    "        #print(\"propagazione sul decoder\")\n",
    "        decoder_parameters = list(self.decoder.parameters())\n",
    "        decoder_weight_list = [w for w in decoder_parameters if len(w.shape) == 2]\n",
    "        decoder_biases_list = [b for b in decoder_parameters if len(b.shape) == 1]\n",
    "        xtildedot = self.t_derivative(z, zdot_hat, decoder_weight_list, decoder_biases_list, activation='sigmoid')    \n",
    "        \n",
    "        return xtilde, xtildedot, z, zdot, zdot_hat\n",
    "\n",
    "    def loss_function(self, x, xdot, xtilde, xtildedot, zdot, zdot_hat,XI):\n",
    "        mse = nn.MSELoss()\n",
    "        alpha1 = 5e-4\n",
    "        alpha2 = 5e-5\n",
    "        alpha3 = 0\n",
    "\n",
    "\n",
    "        loss = {}\n",
    "        loss['recon_loss'] = mse(x, xtilde) #errore di ricostruzione \n",
    "        loss ['sindy_loss_x'] = mse(xdot, xtildedot) \n",
    "        loss ['sindy_loss_z'] = mse(zdot, zdot_hat) \n",
    "        loss['sindy_regular_loss'] = torch.sum(torch.abs(XI)) #norma L1 degli XI\n",
    "        loss['tot'] = loss['recon_loss'] + alpha1*loss['sindy_loss_x'] + alpha2*loss['sindy_loss_z'] + alpha3*loss['sindy_regular_loss']\n",
    "        tot = loss['tot']\n",
    "        return tot, loss\n",
    "    \n",
    "    def forward(self, x, xdot):\n",
    "        return self.compute_quantities(x, xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c4mBzZuzxPYP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = X.shape[1]\n",
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sOuh7zJeoRmH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (fc1): Linear(in_features=2601, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "Linear(in_features=2601, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(input_size,latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8r_OGkXz2Mgl",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.SINDyLibrary.number_candidate_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.SINDyLibrary.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZqwZy2XmMum"
   },
   "source": [
    "# Parameters and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zrBLCBxXTPaa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequential_threshold(t):\n",
    "    if (t % seq_thres == 0 and t>1):\n",
    "        model.XI_coefficient_mask = torch.abs(model.XI) > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(t):\n",
    "    if SAVE == True:\n",
    "        if t % saving_rate == 0 and t > 0:\n",
    "            print('salvataggio a ',t,\"epoche\")\n",
    "            f1 = path + 'model' + '_' + str(t) + 'epochs' + '.pt'\n",
    "            torch.save({\n",
    "            'epoch': t,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': model.configure_optimizers().state_dict(),\n",
    "            'loss': model.loss_function,\n",
    "            'sindy_coefficients': model.XI,\n",
    "            'coefficient_mask' : model.XI_coefficient_mask\n",
    "            }, f1)\n",
    "            \n",
    "            XI = model.XI.cpu().detach().numpy()\n",
    "            np.save(path + 'model' + '_' + str(t) + 'epochs' + '.npy',XI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_model(t):\n",
    "    f = open(path + 'model_equation' + '_' + str(t) + '.txt', 'w')\n",
    "    coefficient_mask = model.XI_coefficient_mask.cpu().detach().numpy()\n",
    "    XI = model.XI.cpu().detach().numpy()\n",
    "    feature_list = model.SINDyLibrary.get_feature_names()\n",
    "    for j in range(latent_dim):\n",
    "        for i in range(len(feature_list)-1):\n",
    "            coeff = XI[i][j] * coefficient_mask[i][j]\n",
    "            if  coeff >= 0.1 and coeff != 0:\n",
    "                #print(f\"dz{j} = {coeff:.4f} {feature_list[i]}\")\n",
    "                f.write(f\"dz{j} = {coeff:.4f} {feature_list[i]}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training step for one epoch\n",
    "def train_step(loss_list):\n",
    "        \n",
    "    #queste serve per le loss di ogni batch\n",
    "    loss_epoch = {}\n",
    "    loss_epoch['recon_loss'] = []\n",
    "    loss_epoch['sindy_loss_x'] = []\n",
    "    loss_epoch['sindy_loss_z'] = []\n",
    "    loss_epoch['sindy_regular_loss'] = []\n",
    "    loss_epoch['tot'] = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X,Xdot) in enumerate (train_loader):\n",
    "        X.to(device) #per passarlo alla gpu\n",
    "        Xdot.to(device)\n",
    "        #forward pass \n",
    "        xtilde, xtildedot, z, zdot, zdot_hat = model(X,Xdot)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer = model.configure_optimizers()\n",
    "        loss, loss_dict = model.loss_function(X, Xdot, xtilde, xtildedot, zdot, zdot_hat,model.XI)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for key in loss_epoch.keys():\n",
    "            loss_epoch[key].append(loss_dict[key].item())\n",
    "        \n",
    "        \n",
    "    for key in loss_epoch.keys():\n",
    "            loss_list[key].append(sum(loss_epoch[key])/len(loss_epoch[key]))\n",
    "            \n",
    "    del loss_epoch,loss_dict,xtilde, xtildedot, z, zdot, zdot_hat,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(loss_val_list):\n",
    "    loss_val_epoch = []\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    for batch, (X,Xdot) in enumerate (val_loader):\n",
    "        X.to(device) #per passarlo alla gpu\n",
    "        Xdot.to(device)\n",
    "        #forward pass \n",
    "        xtilde, xtildedot, z, zdot, zdot_hat = model(X,Xdot)\n",
    "        # validation loss \n",
    "        loss, _ = model.loss_function(X, Xdot, xtilde, xtildedot, zdot, zdot_hat,model.XI)\n",
    "        loss_val_epoch.append(loss.item())\n",
    "        del  xtilde, xtildedot, z, zdot, zdot_hat,loss\n",
    "\n",
    "    loss_val_list.append(sum(loss_val_epoch)/len(loss_val_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(t):\n",
    "    if t % loss_rate == 0:\n",
    "        print()\n",
    "        print(f\"Epoch {t}\\n-------------------------------\")\n",
    "        for key in loss_list.keys():\n",
    "            temp = loss_list[key]\n",
    "            print(f'{key} of epoch {t}: {temp[-1]:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION = False\n",
    "SAVE = True\n",
    "\n",
    "epochs = 2000\n",
    "path = '../model_zero/'\n",
    "\n",
    "seq_thres = 100   # numero epoche ogni quanto fare seq thres\n",
    "\n",
    "equation_rate = 100 #print delle equazioni trovate\n",
    "\n",
    "saving_rate = 100 # numero epoche in cui salvo il modello e gli XI\n",
    "\n",
    "loss_rate = 50  #print delle loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "recon_loss of epoch 50: 6.869e-03\n",
      "sindy_loss_x of epoch 50: 2.456e-01\n",
      "sindy_loss_z of epoch 50: 1.816e+00\n",
      "sindy_regular_loss of epoch 50: 1.924e+01\n",
      "tot of epoch 50: 7.083e-03\n",
      "epoch: 51\n",
      "epoch: 52\n",
      "epoch: 53\n",
      "epoch: 54\n",
      "epoch: 55\n",
      "epoch: 56\n",
      "epoch: 57\n",
      "epoch: 58\n",
      "epoch: 59\n",
      "epoch: 60\n",
      "epoch: 61\n",
      "epoch: 62\n",
      "epoch: 63\n",
      "epoch: 64\n",
      "epoch: 65\n",
      "epoch: 66\n",
      "epoch: 67\n",
      "epoch: 68\n",
      "epoch: 69\n",
      "epoch: 70\n",
      "epoch: 71\n",
      "epoch: 72\n",
      "epoch: 73\n",
      "epoch: 74\n",
      "epoch: 75\n",
      "epoch: 76\n",
      "epoch: 77\n",
      "epoch: 78\n",
      "epoch: 79\n",
      "epoch: 80\n",
      "epoch: 81\n",
      "epoch: 82\n",
      "epoch: 83\n",
      "epoch: 84\n",
      "epoch: 85\n",
      "epoch: 86\n",
      "epoch: 87\n",
      "epoch: 88\n",
      "epoch: 89\n",
      "epoch: 90\n",
      "epoch: 91\n",
      "epoch: 92\n",
      "epoch: 93\n",
      "epoch: 94\n",
      "epoch: 95\n",
      "epoch: 96\n",
      "epoch: 97\n",
      "epoch: 98\n",
      "epoch: 99\n",
      "epoch: 100\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "recon_loss of epoch 100: 6.010e-03\n",
      "sindy_loss_x of epoch 100: 2.426e-01\n",
      "sindy_loss_z of epoch 100: 3.215e-01\n",
      "sindy_regular_loss of epoch 100: 1.840e+01\n",
      "tot of epoch 100: 6.148e-03\n",
      "salvataggio a  100 epoche\n",
      "epoch: 101\n",
      "epoch: 102\n",
      "epoch: 103\n",
      "epoch: 104\n",
      "epoch: 105\n",
      "epoch: 106\n",
      "epoch: 107\n",
      "epoch: 108\n",
      "epoch: 109\n",
      "epoch: 110\n",
      "epoch: 111\n",
      "epoch: 112\n",
      "epoch: 113\n",
      "epoch: 114\n",
      "epoch: 115\n",
      "epoch: 116\n",
      "epoch: 117\n",
      "epoch: 118\n",
      "epoch: 119\n",
      "epoch: 120\n",
      "epoch: 121\n",
      "epoch: 122\n",
      "epoch: 123\n",
      "epoch: 124\n",
      "epoch: 125\n",
      "epoch: 126\n",
      "epoch: 127\n",
      "epoch: 128\n",
      "epoch: 129\n"
     ]
    }
   ],
   "source": [
    "loss_list = {}\n",
    "loss_list['recon_loss'] = []\n",
    "loss_list['sindy_loss_x'] = []\n",
    "loss_list['sindy_loss_z'] = []\n",
    "loss_list['sindy_regular_loss'] = []\n",
    "loss_list['tot'] = []\n",
    "loss_val_list = []\n",
    "\n",
    "\n",
    "while t <= epochs:\n",
    "    print(\"epoch:\",t)\n",
    "    train_step(loss_list)\n",
    "    \n",
    "    validation_step(loss_val_list)\n",
    "    \n",
    "    print_loss(t)\n",
    "\n",
    "    save_model(t)\n",
    "    \n",
    "    sequential_threshold(t)\n",
    "    \n",
    "    if t % equation_rate == 0 and t>1:\n",
    "        print_model(t)\n",
    "        data = pd.DataFrame.from_dict(loss_list)\n",
    "        data.to_csv(path + 'data_'+str(t)+'epochs'+'.csv')\n",
    "    t = t + 1\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('XI',\n",
       "  Parameter containing:\n",
       "  tensor([[0.4375, 0.4209, 0.3701, 0.3860, 0.3989, 0.3715, 0.3485, 0.3876, 0.3717,\n",
       "           0.4091],\n",
       "          [0.9913, 0.9912, 0.9911, 0.9912, 0.9912, 0.9912, 0.9913, 0.9912, 0.9912,\n",
       "           0.9913],\n",
       "          [0.7874, 0.7868, 0.7729, 0.7755, 0.7793, 0.7734, 0.7677, 0.7731, 0.7674,\n",
       "           0.7770],\n",
       "          [0.4513, 0.4297, 0.3819, 0.3916, 0.4072, 0.3859, 0.3528, 0.3956, 0.3839,\n",
       "           0.4132],\n",
       "          [0.4363, 0.4199, 0.3730, 0.3764, 0.4048, 0.3673, 0.3367, 0.3791, 0.3619,\n",
       "           0.3978],\n",
       "          [0.3589, 0.3444, 0.2730, 0.2964, 0.3347, 0.2954, 0.2785, 0.2919, 0.2314,\n",
       "           0.3192],\n",
       "          [0.4046, 0.3734, 0.3357, 0.3536, 0.3719, 0.3413, 0.3317, 0.3615, 0.3221,\n",
       "           0.3830],\n",
       "          [0.4402, 0.4258, 0.3802, 0.3923, 0.4063, 0.3844, 0.3613, 0.3944, 0.3851,\n",
       "           0.4128],\n",
       "          [0.3338, 0.3058, 0.2723, 0.2867, 0.2933, 0.2960, 0.2685, 0.3009, 0.2657,\n",
       "           0.3140],\n",
       "          [0.4326, 0.4221, 0.3717, 0.3843, 0.3990, 0.3741, 0.3556, 0.3838, 0.3755,\n",
       "           0.4055],\n",
       "          [0.4404, 0.4085, 0.3563, 0.3706, 0.3853, 0.3646, 0.3328, 0.3818, 0.3585,\n",
       "           0.4033],\n",
       "          [1.0087, 1.0088, 1.0089, 1.0088, 1.0088, 1.0088, 1.0087, 1.0088, 1.0088,\n",
       "           1.0087],\n",
       "          [1.2126, 1.2132, 1.2271, 1.2245, 1.2206, 1.2266, 1.2323, 1.2269, 1.2326,\n",
       "           1.2230],\n",
       "          [1.5490, 1.5704, 1.6184, 1.6087, 1.5931, 1.6142, 1.6472, 1.6048, 1.6164,\n",
       "           1.5872],\n",
       "          [1.5631, 1.5796, 1.6269, 1.6235, 1.5952, 1.6325, 1.6635, 1.6207, 1.6373,\n",
       "           1.6019],\n",
       "          [1.6402, 1.6551, 1.7268, 1.7034, 1.6650, 1.7044, 1.7219, 1.7078, 1.7680,\n",
       "           1.6802],\n",
       "          [1.5964, 1.6269, 1.6654, 1.6473, 1.6296, 1.6587, 1.6698, 1.6395, 1.6786,\n",
       "           1.6184],\n",
       "          [1.5600, 1.5744, 1.6202, 1.6079, 1.5941, 1.6159, 1.6391, 1.6058, 1.6151,\n",
       "           1.5875],\n",
       "          [1.6684, 1.6963, 1.7310, 1.7157, 1.7095, 1.7068, 1.7355, 1.7023, 1.7378,\n",
       "           1.6880],\n",
       "          [1.5675, 1.5781, 1.6284, 1.6158, 1.6011, 1.6260, 1.6445, 1.6162, 1.6249,\n",
       "           1.5945],\n",
       "          [1.5599, 1.5915, 1.6440, 1.6296, 1.6145, 1.6360, 1.6670, 1.6185, 1.6419,\n",
       "           1.5969],\n",
       "          [0.4375, 0.4209, 0.3701, 0.3860, 0.3989, 0.3715, 0.3485, 0.3876, 0.3717,\n",
       "           0.4091],\n",
       "          [0.4375, 0.4209, 0.3701, 0.3860, 0.3989, 0.3715, 0.3485, 0.3876, 0.3717,\n",
       "           0.4091],\n",
       "          [0.4358, 0.4197, 0.3690, 0.3847, 0.3974, 0.3701, 0.3475, 0.3863, 0.3698,\n",
       "           0.4080],\n",
       "          [0.4379, 0.4210, 0.3703, 0.3862, 0.3991, 0.3718, 0.3487, 0.3880, 0.3723,\n",
       "           0.4095],\n",
       "          [0.4375, 0.4209, 0.3702, 0.3860, 0.3990, 0.3715, 0.3485, 0.3876, 0.3720,\n",
       "           0.4092],\n",
       "          [0.4379, 0.4211, 0.3706, 0.3861, 0.3988, 0.3719, 0.3482, 0.3876, 0.3725,\n",
       "           0.4088],\n",
       "          [0.4371, 0.4195, 0.3684, 0.3839, 0.3972, 0.3692, 0.3452, 0.3862, 0.3683,\n",
       "           0.4075],\n",
       "          [0.4376, 0.4211, 0.3706, 0.3863, 0.3992, 0.3719, 0.3488, 0.3879, 0.3723,\n",
       "           0.4093],\n",
       "          [0.4374, 0.4205, 0.3696, 0.3855, 0.3987, 0.3711, 0.3472, 0.3874, 0.3705,\n",
       "           0.4087],\n",
       "          [0.4371, 0.4209, 0.3704, 0.3863, 0.3992, 0.3713, 0.3488, 0.3875, 0.3718,\n",
       "           0.4092],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000],\n",
       "          [0.9963, 0.9963, 0.9962, 0.9963, 0.9963, 0.9963, 0.9963, 0.9962, 0.9962,\n",
       "           0.9963],\n",
       "          [0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998,\n",
       "           0.9998],\n",
       "          [0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
       "           0.9999],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000],\n",
       "          [0.9966, 0.9965, 0.9965, 0.9965, 0.9965, 0.9965, 0.9966, 0.9965, 0.9965,\n",
       "           0.9965],\n",
       "          [0.9817, 0.9817, 0.9810, 0.9811, 0.9814, 0.9809, 0.9805, 0.9810, 0.9807,\n",
       "           0.9813],\n",
       "          [0.9447, 0.9448, 0.9408, 0.9414, 0.9425, 0.9409, 0.9387, 0.9407, 0.9392,\n",
       "           0.9422],\n",
       "          [0.8852, 0.8855, 0.8780, 0.8791, 0.8814, 0.8774, 0.8741, 0.8774, 0.8738,\n",
       "           0.8785],\n",
       "          [0.9488, 0.9491, 0.9447, 0.9455, 0.9467, 0.9444, 0.9425, 0.9445, 0.9427,\n",
       "           0.9453],\n",
       "          [0.9970, 0.9965, 0.9950, 0.9962, 0.9960, 0.9956, 0.9959, 0.9961, 0.9958,\n",
       "           0.9971],\n",
       "          [0.9949, 0.9948, 0.9948, 0.9947, 0.9948, 0.9948, 0.9947, 0.9948, 0.9948,\n",
       "           0.9949],\n",
       "          [0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982, 0.9982,\n",
       "           0.9982],\n",
       "          [0.9864, 0.9865, 0.9844, 0.9843, 0.9852, 0.9842, 0.9833, 0.9840, 0.9830,\n",
       "           0.9842],\n",
       "          [0.9750, 0.9751, 0.9730, 0.9729, 0.9737, 0.9730, 0.9718, 0.9727, 0.9718,\n",
       "           0.9735],\n",
       "          [0.4534, 0.4307, 0.3781, 0.3871, 0.4061, 0.3818, 0.3417, 0.3942, 0.3829,\n",
       "           0.4092],\n",
       "          [0.4830, 0.4645, 0.4120, 0.4055, 0.4350, 0.4061, 0.3661, 0.4132, 0.4165,\n",
       "           0.4245],\n",
       "          [0.2490, 0.3153, 0.2950, 0.2717, 0.2996, 0.3111, 0.3353, 0.2603, 0.2102,\n",
       "           0.2581],\n",
       "          [0.3953, 0.4063, 0.3318, 0.3517, 0.3400, 0.3881, 0.3260, 0.3617, 0.3483,\n",
       "           0.3432],\n",
       "          [0.4394, 0.4372, 0.3775, 0.3924, 0.3951, 0.3992, 0.3649, 0.3924, 0.3902,\n",
       "           0.4007],\n",
       "          [0.8912, 0.8864, 0.8727, 0.8883, 0.8909, 0.8790, 0.8856, 0.8867, 0.8760,\n",
       "           0.8990],\n",
       "          [0.4562, 0.4578, 0.3980, 0.4065, 0.4142, 0.4144, 0.4097, 0.3952, 0.4041,\n",
       "           0.4212],\n",
       "          [0.4526, 0.4235, 0.3682, 0.3686, 0.3894, 0.3797, 0.3389, 0.3860, 0.3680,\n",
       "           0.4023],\n",
       "          [0.4258, 0.4131, 0.3669, 0.3670, 0.4033, 0.3611, 0.3354, 0.3713, 0.3403,\n",
       "           0.3837],\n",
       "          [0.2591, 0.3285, 0.3580, 0.3029, 0.3201, 0.3418, 0.3795, 0.2802, 0.2237,\n",
       "           0.2203],\n",
       "          [0.5589, 0.5408, 0.4962, 0.5363, 0.5086, 0.5291, 0.4626, 0.5218, 0.4928,\n",
       "           0.5476],\n",
       "          [0.4642, 0.4645, 0.3944, 0.3954, 0.4172, 0.4007, 0.3703, 0.3928, 0.4318,\n",
       "           0.4187],\n",
       "          [0.5456, 0.5626, 0.5562, 0.5410, 0.5245, 0.5732, 0.5227, 0.5347, 0.5240,\n",
       "           0.5309],\n",
       "          [0.4443, 0.4370, 0.3719, 0.3741, 0.4040, 0.3719, 0.3388, 0.3781, 0.3767,\n",
       "           0.3955],\n",
       "          [0.4306, 0.3990, 0.3577, 0.3480, 0.3905, 0.3630, 0.3278, 0.3795, 0.2914,\n",
       "           0.3691],\n",
       "          [0.2186, 0.2960, 0.2911, 0.2580, 0.2785, 0.2943, 0.3163, 0.2372, 0.1954,\n",
       "           0.2231],\n",
       "          [0.6578, 0.6297, 0.5595, 0.6521, 0.6165, 0.6118, 0.6324, 0.6340, 0.6003,\n",
       "           0.6910],\n",
       "          [0.6528, 0.6333, 0.6122, 0.6176, 0.6457, 0.5900, 0.5600, 0.6003, 0.6123,\n",
       "           0.6301],\n",
       "          [0.9376, 0.9323, 0.9178, 0.9258, 0.9282, 0.9237, 0.9205, 0.9279, 0.9156,\n",
       "           0.9360],\n",
       "          [0.2806, 0.3908, 0.4073, 0.3629, 0.4081, 0.3959, 0.4920, 0.3501, 0.2805,\n",
       "           0.3213],\n",
       "          [0.4447, 0.4214, 0.3550, 0.3638, 0.3899, 0.3744, 0.2965, 0.3571, 0.3401,\n",
       "           0.3887],\n",
       "          [0.4189, 0.3832, 0.3492, 0.3619, 0.3864, 0.3469, 0.3490, 0.3708, 0.3353,\n",
       "           0.3972],\n",
       "          [0.4220, 0.4158, 0.3913, 0.3767, 0.3951, 0.3980, 0.3566, 0.3879, 0.3728,\n",
       "           0.3834],\n",
       "          [0.4550, 0.4622, 0.4512, 0.4388, 0.4223, 0.4705, 0.4152, 0.4379, 0.4253,\n",
       "           0.4317],\n",
       "          [0.4175, 0.3886, 0.3806, 0.3578, 0.3950, 0.3610, 0.3766, 0.3716, 0.3524,\n",
       "           0.3910],\n",
       "          [0.5617, 0.5046, 0.4122, 0.5365, 0.4993, 0.4748, 0.4623, 0.5186, 0.4649,\n",
       "           0.5884],\n",
       "          [0.4376, 0.4256, 0.3773, 0.3904, 0.4036, 0.3859, 0.3650, 0.3928, 0.3859,\n",
       "           0.4105],\n",
       "          [0.4424, 0.4814, 0.4707, 0.4545, 0.5071, 0.4792, 0.6003, 0.4793, 0.4384,\n",
       "           0.4991],\n",
       "          [0.4432, 0.4333, 0.3803, 0.3917, 0.4052, 0.3898, 0.3775, 0.3907, 0.3915,\n",
       "           0.4128],\n",
       "          [0.4278, 0.4014, 0.3264, 0.3691, 0.3941, 0.3436, 0.3541, 0.3778, 0.3727,\n",
       "           0.4070],\n",
       "          [0.3653, 0.3661, 0.3579, 0.3428, 0.3427, 0.3747, 0.3507, 0.3579, 0.3364,\n",
       "           0.3478],\n",
       "          [0.9027, 0.9051, 0.8860, 0.8996, 0.9200, 0.8889, 0.9233, 0.9007, 0.8862,\n",
       "           0.9201],\n",
       "          [0.9036, 0.8866, 0.8784, 0.9078, 0.8951, 0.8906, 0.8732, 0.9026, 0.8856,\n",
       "           0.9167],\n",
       "          [0.4360, 0.4264, 0.3629, 0.3794, 0.3954, 0.3713, 0.3551, 0.3808, 0.3764,\n",
       "           0.4019],\n",
       "          [0.4454, 0.4366, 0.3714, 0.3881, 0.4024, 0.3965, 0.4055, 0.3971, 0.3764,\n",
       "           0.4126],\n",
       "          [0.4557, 0.4194, 0.3660, 0.3763, 0.3932, 0.3801, 0.3370, 0.3925, 0.3717,\n",
       "           0.4135]], device='cuda:0', requires_grad=True)),\n",
       " ('encoder.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0380,  0.0211,  0.0124,  ...,  0.0638,  0.0552,  0.0155],\n",
       "          [-0.0179,  0.0184, -0.0154,  ..., -0.0605, -0.0014,  0.0568],\n",
       "          [ 0.0213,  0.0298,  0.0376,  ..., -0.0334, -0.0372, -0.0151],\n",
       "          ...,\n",
       "          [-0.0103, -0.0222, -0.0294,  ...,  0.0424,  0.0161, -0.0135],\n",
       "          [ 0.0932,  0.0486,  0.0631,  ...,  0.0416,  0.0831,  0.1094],\n",
       "          [ 0.0261,  0.0093,  0.0045,  ..., -0.0017,  0.0251, -0.0194]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('encoder.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.1523, 0.0922, 0.0946, 0.1139, 0.1150, 0.0918, 0.0874, 0.0876, 0.0959,\n",
       "          0.1145, 0.1003, 0.1304, 0.0902, 0.1251, 0.1455, 0.1411, 0.1046, 0.0999,\n",
       "          0.1059, 0.1011, 0.1056, 0.0742, 0.0778, 0.1593, 0.0896, 0.1120, 0.0976,\n",
       "          0.0785, 0.0963, 0.1017, 0.2241, 0.0964, 0.0956, 0.2460, 0.0833, 0.1058,\n",
       "          0.0921, 0.0949, 0.0893, 0.1116, 0.1032, 0.0778, 0.0834, 0.1111, 0.0922,\n",
       "          0.1079, 0.1049, 0.0809, 0.1161, 0.1637, 0.0926, 0.0960, 0.1010, 0.0865,\n",
       "          0.1012, 0.1019, 0.0871, 0.0662, 0.0889, 0.1151, 0.0953, 0.0793, 0.1463,\n",
       "          0.1086, 0.0913, 0.1020, 0.0961, 0.0821, 0.0926, 0.0936, 0.1019, 0.0985,\n",
       "          0.0935, 0.1021, 0.0816, 0.0825, 0.2254, 0.0883, 0.0742, 0.0808, 0.1048,\n",
       "          0.0931, 0.0944, 0.2267, 0.0982, 0.0904, 0.0953, 0.0986, 0.1039, 0.0987,\n",
       "          0.0923, 0.0933, 0.1024, 0.0943, 0.0988, 0.0945, 0.1658, 0.0970, 0.0961,\n",
       "          0.0968, 0.0950, 0.1126, 0.0939, 0.0938, 0.0993, 0.0907, 0.0949, 0.1135,\n",
       "          0.0930, 0.0777, 0.1034, 0.0882, 0.1095, 0.0981, 0.1616, 0.1584, 0.0971,\n",
       "          0.0694, 0.0957, 0.1221, 0.0968, 0.0885, 0.1292, 0.0862, 0.1232, 0.1408,\n",
       "          0.1065, 0.0988], device='cuda:0', requires_grad=True)),\n",
       " ('encoder.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0051, -0.1700,  0.0053,  ...,  0.1768,  0.0896, -0.0907],\n",
       "          [-0.1225,  0.1475,  0.1030,  ...,  0.0661,  0.0736, -0.0312],\n",
       "          [-0.0275, -0.1691, -0.0632,  ..., -0.0934,  0.0133, -0.1699],\n",
       "          ...,\n",
       "          [ 0.1609,  0.2213, -0.0222,  ...,  0.0752,  0.1693, -0.0422],\n",
       "          [-0.1611, -0.0441, -0.0567,  ..., -0.1916, -0.0624, -0.0582],\n",
       "          [-0.1540,  0.0164,  0.0043,  ..., -0.0714,  0.0613,  0.1331]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('encoder.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.1014, 0.1024, 0.0877, 0.1011, 0.1023, 0.1023, 0.0661, 0.0952, 0.0994,\n",
       "          0.1041, 0.0818, 0.0968, 0.1023, 0.1019, 0.1083, 0.0975, 0.1021, 0.1149,\n",
       "          0.0834, 0.1053, 0.0933, 0.1148, 0.0852, 0.0829, 0.1004, 0.0972, 0.1050,\n",
       "          0.0999, 0.1161, 0.0980, 0.1004, 0.1003, 0.0786, 0.1035, 0.1180, 0.1107,\n",
       "          0.0993, 0.0997, 0.0884, 0.1026, 0.0993, 0.0936, 0.1032, 0.1103, 0.1013,\n",
       "          0.0918, 0.0941, 0.1018, 0.1004, 0.0925, 0.1138, 0.0876, 0.0986, 0.0874,\n",
       "          0.1126, 0.0869, 0.0996, 0.1033, 0.0999, 0.0972, 0.1047, 0.0975, 0.1017,\n",
       "          0.0915], device='cuda:0', requires_grad=True)),\n",
       " ('encoder.fc3.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1441, -0.0324,  0.3354,  ...,  0.2084, -0.1720,  0.1288],\n",
       "          [ 0.1369, -0.2077,  0.0911,  ..., -0.3342, -0.1321,  0.2162],\n",
       "          [-0.0436, -0.0411,  0.2730,  ...,  0.5428, -0.3362, -0.3190],\n",
       "          ...,\n",
       "          [-0.2645, -0.0416,  0.1231,  ..., -0.0821,  0.0725, -0.0451],\n",
       "          [-0.3814,  0.2409,  0.0403,  ..., -0.4937,  0.4452,  0.5823],\n",
       "          [ 0.5137,  0.0279,  0.1771,  ...,  0.0507, -0.0143,  0.2811]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('encoder.fc3.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1019,  0.0981, -0.3533,  0.1001,  0.1022,  0.0996,  0.0965,  0.1008,\n",
       "           0.1016, -0.0934,  0.1014,  0.1008,  0.1016,  0.0997, -0.3239,  0.0722,\n",
       "           0.0542,  0.0984,  0.1052,  0.1004,  0.1002,  0.1005,  0.0966,  0.0998,\n",
       "           0.0658,  0.0983, -0.1404,  0.1024,  0.0910,  0.0847,  0.1046,  0.1003],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('encoder.fc4.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-2.1452e-01, -1.3711e-01, -1.2028e+00,  1.5735e-01,  9.5983e-02,\n",
       "           -3.3089e-01,  3.4415e-01,  5.1778e-01,  1.1952e-01, -8.5045e-01,\n",
       "           -5.6112e-01, -1.6077e-01, -6.4583e-01,  1.0122e-01, -7.2961e-01,\n",
       "            6.7603e-01,  6.6113e-01, -1.2364e-01,  3.1302e-01, -4.5140e-01,\n",
       "            1.4532e-01,  2.3057e-01, -4.4154e-01, -6.3085e-01,  4.0573e-01,\n",
       "            6.8788e-03, -6.7890e-01,  3.1045e-01,  1.5667e-01, -2.0720e-01,\n",
       "            5.1734e-01,  2.2000e-01],\n",
       "          [ 5.9251e-02,  1.1603e-01, -9.2872e-01,  3.8426e-01,  1.4956e-01,\n",
       "           -2.3811e-01,  4.7998e-01, -5.8095e-02, -8.9902e-02, -7.3307e-01,\n",
       "           -7.3317e-01,  2.1693e-01, -3.2050e-01,  4.9907e-02, -5.5029e-01,\n",
       "            2.2189e-01,  9.1651e-01,  2.3474e-01, -1.9897e-01, -1.7033e-01,\n",
       "            4.5066e-01,  2.3570e-01, -1.6739e-01, -7.2035e-01,  3.7042e-01,\n",
       "            1.0965e-01, -6.4641e-01,  5.2329e-01,  1.5908e-01, -3.4472e-01,\n",
       "            2.8620e-01, -3.9357e-02],\n",
       "          [ 9.0878e-02, -2.1013e-01, -8.3126e-01,  1.7409e-01, -3.0426e-01,\n",
       "           -9.1755e-02,  4.1012e-01,  3.8823e-01,  3.7137e-02, -4.5319e-01,\n",
       "            4.7087e-03,  1.2490e-01,  2.0221e-01,  1.7828e-01, -1.0948e+00,\n",
       "            5.1539e-01,  2.2503e-01, -1.2706e-01, -2.1965e-01, -2.9586e-01,\n",
       "           -6.6188e-02,  4.1285e-02, -3.0982e-01,  5.9029e-02,  4.2911e-01,\n",
       "           -7.6585e-02, -8.8294e-01,  2.2160e-01,  2.0793e-01, -5.7585e-04,\n",
       "            2.3270e-01, -1.8310e-01],\n",
       "          [ 2.7817e-01, -9.1073e-02, -4.4980e-01, -9.6913e-02, -3.7624e-01,\n",
       "           -1.8632e-01,  9.2725e-02, -4.6269e-02, -1.5751e-01, -5.2463e-01,\n",
       "            3.1756e-02, -3.7460e-02, -5.6286e-02,  1.1135e-01, -1.0861e+00,\n",
       "            7.8111e-01,  1.1486e+00,  1.7467e-01, -1.2316e-01,  1.5402e-01,\n",
       "            3.2587e-01,  3.1055e-01,  2.5576e-01,  1.5298e-01,  7.3184e-01,\n",
       "            1.1850e-01, -4.9268e-01,  1.8170e-02,  3.5115e-01, -1.7284e-01,\n",
       "            3.9151e-01,  8.0470e-04],\n",
       "          [ 2.2223e-01,  5.9350e-02, -5.6105e-01, -1.5083e-01,  5.6783e-02,\n",
       "           -3.2335e-01,  4.7053e-02,  1.0064e-01,  7.2566e-02, -8.5154e-01,\n",
       "           -4.5108e-02, -4.5370e-01,  1.0253e-02, -1.5039e-01, -8.0448e-01,\n",
       "            4.4009e-01,  6.6634e-01,  2.1137e-01,  2.1327e-01, -2.3173e-01,\n",
       "            2.9326e-01, -2.5531e-01,  1.0937e-01, -4.7008e-01,  8.5151e-01,\n",
       "            2.9225e-02, -4.5873e-01, -8.5837e-02,  6.3273e-01, -2.4897e-01,\n",
       "            8.6271e-01,  3.4684e-01],\n",
       "          [ 4.6663e-01,  4.3576e-01, -6.3740e-01, -3.1727e-02,  5.7555e-01,\n",
       "            2.7947e-02,  4.1316e-01, -5.3937e-01, -5.3249e-01, -2.9560e-01,\n",
       "            5.7756e-02,  1.0684e-01,  8.8702e-02,  2.5501e-01, -7.3630e-01,\n",
       "            3.0928e-01,  1.1770e+00,  1.2360e-01, -1.0716e-01, -7.9953e-03,\n",
       "            1.7787e-01, -1.1121e-01, -2.5588e-01, -1.1463e-01,  1.6375e-01,\n",
       "            2.4532e-01, -5.5089e-01, -1.6961e-02,  5.9316e-02,  2.2651e-01,\n",
       "            2.7630e-01, -1.1647e-01],\n",
       "          [-4.1093e-03,  2.7527e-01, -4.2525e-01,  5.5784e-01,  1.6517e-01,\n",
       "            2.1678e-01, -1.4874e-01, -7.3995e-02, -1.4827e-01, -4.0530e-01,\n",
       "           -2.3969e-01,  3.8006e-01,  1.0206e-01,  2.1369e-01, -1.1657e-01,\n",
       "            1.3541e-01,  1.0897e+00,  1.8164e-01, -1.1413e-01, -2.8762e-01,\n",
       "           -6.4849e-02, -1.0649e-01, -2.5914e-01,  2.7588e-01,  3.6806e-01,\n",
       "            1.3656e-01, -5.5006e-01, -6.4760e-01,  7.3428e-03,  1.6099e-01,\n",
       "            1.0578e+00, -7.6914e-03],\n",
       "          [-1.2177e-01,  1.5863e-01, -8.7989e-01,  1.2760e-01, -1.9718e-01,\n",
       "           -1.0830e-01,  7.3500e-01, -1.0080e-01, -2.1777e-01, -7.2876e-01,\n",
       "           -2.5216e-01,  4.0536e-02, -2.0153e-01,  8.6621e-02, -8.1930e-01,\n",
       "            6.0316e-01,  7.3381e-01,  3.2613e-01,  1.0371e-01, -2.5563e-01,\n",
       "            7.2256e-02, -2.6339e-01,  9.7185e-03, -5.0068e-01,  4.1857e-01,\n",
       "           -3.2928e-01, -4.0715e-02, -1.1464e-01,  3.3483e-01,  4.3093e-01,\n",
       "            3.5912e-01,  6.7041e-02],\n",
       "          [ 3.0152e-01,  5.9707e-02, -8.2654e-01,  5.6115e-02,  3.2390e-01,\n",
       "            1.8918e-01,  5.4868e-01,  1.3604e-02, -1.1586e-01, -4.5077e-01,\n",
       "            1.3799e-01,  2.0939e-01, -1.1253e-01, -8.4811e-02, -7.8966e-01,\n",
       "            1.3504e-01,  8.4060e-01, -5.9244e-01,  2.6697e-01,  2.5454e-01,\n",
       "            2.5532e-01, -3.4952e-02,  1.9604e-01,  1.7024e-01,  4.7845e-01,\n",
       "            7.9023e-01, -7.5561e-01, -3.5916e-02,  2.1177e-01, -5.0267e-01,\n",
       "            1.0365e-01, -2.1276e-01],\n",
       "          [ 8.5367e-02, -4.3737e-01, -5.1819e-01,  2.4381e-01,  1.0365e-01,\n",
       "           -2.8646e-01, -8.4282e-02,  3.7153e-01, -9.6923e-02, -6.4488e-01,\n",
       "            1.3509e-01,  1.0269e-01, -3.1620e-02, -2.1064e-01, -1.1014e+00,\n",
       "            7.5620e-01,  1.1119e+00,  2.8067e-01,  2.4990e-01, -2.3275e-01,\n",
       "            4.3179e-01,  9.5098e-02,  4.6873e-02, -1.0962e-01,  6.9346e-01,\n",
       "            3.4641e-01, -5.4556e-01, -2.4474e-02, -1.1969e-01, -2.5650e-01,\n",
       "            3.7691e-01,  4.6575e-01]], device='cuda:0', requires_grad=True)),\n",
       " ('encoder.fc4.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0920, -0.0185,  0.1020,  0.1061,  0.0763,  0.1010,  0.0978,  0.0664,\n",
       "           0.0963,  0.0970], device='cuda:0', requires_grad=True)),\n",
       " ('decoder.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 4.3902e-01,  3.4117e-01, -5.7872e-01,  1.9900e-01, -6.9008e-01,\n",
       "           -7.2958e-01, -8.9796e-01,  2.6377e-01,  3.6365e-01, -9.7565e-02],\n",
       "          [-2.4335e-01,  6.3815e-01, -1.7488e-02,  1.9257e-01,  6.2904e-01,\n",
       "           -9.4527e-01, -6.1210e-01, -2.3169e-01, -7.4455e-02, -2.6934e-01],\n",
       "          [ 7.4445e-01,  5.4609e-01,  5.7423e-01,  1.5605e-01,  5.0610e-01,\n",
       "           -6.0785e-01, -1.6749e-01,  9.9527e-01,  4.2885e-01,  7.9802e-02],\n",
       "          [ 1.0839e-01, -6.2554e-01, -2.9089e-01,  1.7803e-03,  4.2547e-01,\n",
       "           -4.8677e-01, -4.9230e-01, -2.0411e-01,  1.6824e-01, -3.3564e-01],\n",
       "          [-4.4988e-01, -1.0943e+00, -3.8500e-01,  8.8732e-02, -4.0326e-01,\n",
       "           -4.2734e-01, -7.3186e-03,  2.3997e-01,  2.1261e-01, -1.9312e-01],\n",
       "          [-2.7052e-01, -3.5333e-01, -4.5834e-01, -3.4842e-01, -2.9820e-01,\n",
       "           -5.9314e-01, -9.4702e-01,  6.4800e-01, -4.0961e-01,  1.6535e-01],\n",
       "          [-6.4432e-01,  4.3002e-01, -8.8745e-02, -5.3937e-01,  4.5473e-01,\n",
       "            6.5075e-01,  5.0075e-01,  7.3098e-01, -6.4738e-02,  3.8256e-01],\n",
       "          [ 6.6191e-01, -6.9622e-01,  6.6994e-02, -2.3095e-01, -9.0046e-02,\n",
       "           -3.5191e-01, -3.0965e-01,  5.7085e-03,  6.1903e-01,  3.9816e-01],\n",
       "          [ 5.1018e-02,  1.7589e-01, -5.7510e-01,  1.4253e+00,  4.1969e-01,\n",
       "            1.4367e-01, -5.1764e-01,  5.2895e-01,  3.8622e-01,  7.0796e-01],\n",
       "          [ 3.9427e-01, -5.2697e-01, -4.2873e-01,  2.0671e-01,  2.3897e-01,\n",
       "           -2.0857e-01, -6.1990e-01,  3.0647e-01,  5.6592e-02, -4.8866e-01],\n",
       "          [ 1.9419e-01,  6.1415e-01,  1.7410e-02,  3.2483e-01,  5.5642e-01,\n",
       "           -4.6323e-01,  2.2974e-01,  2.7484e-01,  1.7021e-01, -6.4552e-01],\n",
       "          [-1.5363e-02, -8.3951e-01,  3.0629e-01,  4.5292e-01, -1.5171e-01,\n",
       "            3.5257e-01, -7.4866e-01,  8.2243e-01, -8.0024e-01,  1.0049e-04],\n",
       "          [-5.0226e-02, -3.1610e-01,  1.9519e-01, -1.5555e-01, -2.1538e-01,\n",
       "           -1.3864e-01,  9.6083e-03,  3.2100e-02,  1.3182e-01,  2.0153e-01],\n",
       "          [ 4.5769e-01, -5.5674e-01, -3.5639e-01, -1.9272e-01,  1.4371e-01,\n",
       "            6.2500e-02,  3.1300e-01,  6.0939e-01, -6.4796e-01,  5.8122e-01],\n",
       "          [ 2.7115e-01, -6.5003e-02,  1.9288e-01,  1.1366e-01,  1.6884e-01,\n",
       "            3.0354e-01, -9.4147e-02, -1.7050e-01,  3.6360e-01, -2.4809e-01],\n",
       "          [-1.3224e-01, -9.6705e-01,  1.4364e+00,  6.4211e-01, -4.1833e-01,\n",
       "           -4.6385e-01,  4.9641e-01, -3.8815e-01,  4.5403e-01,  6.8022e-02],\n",
       "          [-5.9964e-01, -1.3870e-02, -7.9145e-01,  6.9588e-02,  6.8437e-01,\n",
       "            4.0939e-02, -2.5804e-01,  8.7528e-02,  2.7628e-01, -5.1618e-01],\n",
       "          [-1.1308e-01, -1.0721e+00, -4.8377e-01, -4.1381e-01,  5.1565e-01,\n",
       "           -8.7920e-02, -6.8721e-02, -3.7612e-01,  2.7156e-01,  2.1992e-01],\n",
       "          [-2.3265e-01, -1.3599e-01, -4.4592e-02, -7.6657e-01,  1.9671e-01,\n",
       "            1.4842e-01, -1.1635e-01,  3.3702e-01, -3.4801e-01,  2.4614e-01],\n",
       "          [ 1.0512e+00,  4.7650e-01, -7.0540e-01,  3.5736e-01,  1.3400e-01,\n",
       "            1.6442e-01, -6.2191e-01, -2.9965e-01,  8.5718e-02,  1.8359e-01],\n",
       "          [-6.5198e-03, -2.5443e-01, -7.3517e-01, -2.7401e-01, -4.0887e-01,\n",
       "           -2.8372e-02,  3.3944e-01, -1.1905e-01, -1.9408e-01,  1.9439e-01],\n",
       "          [-1.0537e+00,  9.2327e-01, -4.4458e-01,  4.5592e-01,  5.2449e-01,\n",
       "            1.1169e+00,  3.0550e-02, -4.0283e-02,  3.2599e-02, -7.5681e-01],\n",
       "          [-3.1270e-01, -1.8846e-01,  9.7159e-01,  2.5904e-01,  2.4516e-01,\n",
       "            8.9195e-02, -3.8896e-01, -3.4007e-01, -4.3950e-01, -4.0081e-01],\n",
       "          [ 4.6951e-01, -5.0392e-02,  2.0307e-01,  5.6806e-02,  1.7407e-01,\n",
       "            1.7656e-01, -4.2600e-01,  2.0991e-02,  5.2022e-01,  4.0060e-01],\n",
       "          [-2.9945e-01,  2.2959e-01, -9.2345e-01, -7.3490e-02,  3.7074e-01,\n",
       "            1.5241e-01, -1.5129e-02, -2.7121e-01,  4.0022e-01, -4.5380e-02],\n",
       "          [-3.7706e-01,  2.5430e-01,  3.6735e-01,  1.5501e-01, -6.2001e-01,\n",
       "            5.4139e-01,  1.7220e-01, -2.9875e-01, -1.3926e-01, -3.1929e-01],\n",
       "          [ 7.7411e-01, -2.5692e-01,  7.6876e-02, -5.3340e-02,  5.2735e-01,\n",
       "           -3.6054e-01,  1.1914e-03, -6.7399e-02, -7.4360e-01,  6.4529e-01],\n",
       "          [-1.8246e-01,  3.7242e-01,  3.7801e-01,  5.3935e-01,  2.4961e-01,\n",
       "            4.6907e-01, -5.5597e-01,  7.1210e-02,  4.0101e-01, -2.6881e-01],\n",
       "          [ 2.8650e-02, -4.8592e-01, -1.2227e-01,  3.0130e-01,  1.9558e-01,\n",
       "           -1.0819e-01, -4.0231e-02,  2.3571e-01,  7.5586e-02,  2.2157e-01],\n",
       "          [ 1.4482e-01, -2.8213e-02, -1.8417e-01, -2.2927e-01, -5.1705e-01,\n",
       "            1.6935e-01,  6.9312e-01,  1.1953e-01,  1.5185e-01, -5.6845e-01],\n",
       "          [-2.9912e-01,  3.2695e-01, -1.0011e+00, -9.6495e-03, -1.0573e-01,\n",
       "           -1.2893e+00,  4.6448e-01, -6.8447e-02,  3.2720e-01,  1.8761e-01],\n",
       "          [-5.4116e-01, -2.8046e-01,  2.1288e-02, -7.8828e-02, -4.2631e-01,\n",
       "           -2.3044e-01, -6.0229e-02,  2.4250e-01,  5.3141e-01, -4.2379e-01]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('decoder.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.0979, 0.1214, 0.0930, 0.1669, 0.0968, 0.1719, 0.0835, 0.1135, 0.0711,\n",
       "          0.2464, 0.0821, 0.1374, 0.0848, 0.1089, 0.1100, 0.0604, 0.1115, 0.1318,\n",
       "          0.1365, 0.1420, 0.0719, 0.1171, 0.1533, 0.0816, 0.1267, 0.1024, 0.0780,\n",
       "          0.1077, 0.0835, 0.0868, 0.1089, 0.1370], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('decoder.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0456, -0.3624,  0.0355,  ...,  0.1352,  0.1464,  0.1750],\n",
       "          [ 0.2091,  0.0892,  0.2511,  ...,  0.3932,  0.0389, -0.3141],\n",
       "          [ 0.1577, -0.1456, -0.0264,  ...,  0.2122,  0.0790, -0.2657],\n",
       "          ...,\n",
       "          [ 0.0138,  0.3997, -0.2391,  ..., -0.0089,  0.2425,  0.1476],\n",
       "          [-0.0035,  0.0472,  0.1149,  ...,  0.3704,  0.0065, -0.0848],\n",
       "          [ 0.4543, -0.1089, -0.2672,  ..., -0.5478, -0.1652,  0.0770]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('decoder.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.0753, 0.1098, 0.0967, 0.1322, 0.1071, 0.1311, 0.0945, 0.0928, 0.1467,\n",
       "          0.0373, 0.1004, 0.1091, 0.1613, 0.0966, 0.1453, 0.1380, 0.0164, 0.1066,\n",
       "          0.0980, 0.1525, 0.1540, 0.2087, 0.1039, 0.1108, 0.0841, 0.1771, 0.1667,\n",
       "          0.0971, 0.0900, 0.1194, 0.1028, 0.0645, 0.1291, 0.1179, 0.1000, 0.1149,\n",
       "          0.0954, 0.0731, 0.1061, 0.1162, 0.1005, 0.1625, 0.1041, 0.1718, 0.1095,\n",
       "          0.0908, 0.0614, 0.0887, 0.0775, 0.1206, 0.0706, 0.1449, 0.0861, 0.0604,\n",
       "          0.0838, 0.0861, 0.1235, 0.1626, 0.0773, 0.1190, 0.0871, 0.1165, 0.0825,\n",
       "          0.1640], device='cuda:0', requires_grad=True)),\n",
       " ('decoder.fc3.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.2149,  0.1808,  0.0675,  ...,  0.1196,  0.1520, -0.0053],\n",
       "          [ 0.0159, -0.0669,  0.1241,  ..., -0.1304,  0.1994,  0.1917],\n",
       "          [ 0.0524, -0.4154, -0.0638,  ...,  0.4090,  0.0477, -0.1316],\n",
       "          ...,\n",
       "          [-0.0221,  0.1747, -0.0760,  ...,  0.0790,  0.2180, -0.1015],\n",
       "          [ 0.2879,  0.1870, -0.0270,  ...,  0.0718,  0.0485, -0.0493],\n",
       "          [-0.0510,  0.1174,  0.0066,  ..., -0.3045, -0.2145,  0.1582]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('decoder.fc3.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.1029, 0.0765, 0.1360, 0.0687, 0.1205, 0.1142, 0.1258, 0.0973, 0.0825,\n",
       "          0.0751, 0.1352, 0.1155, 0.1185, 0.1639, 0.0676, 0.0908, 0.1388, 0.1007,\n",
       "          0.1548, 0.1246, 0.0964, 0.0817, 0.0996, 0.1090, 0.1577, 0.1221, 0.0943,\n",
       "          0.0926, 0.1376, 0.1257, 0.1281, 0.1457, 0.0888, 0.1245, 0.0786, 0.1468,\n",
       "          0.1106, 0.1134, 0.1341, 0.1382, 0.1044, 0.1050, 0.1186, 0.0992, 0.1640,\n",
       "          0.0999, 0.1378, 0.1016, 0.0807, 0.0932, 0.1352, 0.1636, 0.1137, 0.1151,\n",
       "          0.1179, 0.0791, 0.0950, 0.0848, 0.1307, 0.0855, 0.0889, 0.1335, 0.1315,\n",
       "          0.1480, 0.1070, 0.1151, 0.1273, 0.1128, 0.1290, 0.1322, 0.1178, 0.1423,\n",
       "          0.1446, 0.0850, 0.1006, 0.1142, 0.1154, 0.1163, 0.1104, 0.1071, 0.1733,\n",
       "          0.1215, 0.1026, 0.1214, 0.1325, 0.1006, 0.1073, 0.1103, 0.1322, 0.0959,\n",
       "          0.1422, 0.1237, 0.1236, 0.1148, 0.1750, 0.1332, 0.1440, 0.1264, 0.1207,\n",
       "          0.1266, 0.1188, 0.0961, 0.0864, 0.1323, 0.1452, 0.1163, 0.1379, 0.1334,\n",
       "          0.1326, 0.1043, 0.0934, 0.1035, 0.1166, 0.0968, 0.1119, 0.1214, 0.1146,\n",
       "          0.1275, 0.1061, 0.1574, 0.1338, 0.1110, 0.1052, 0.1122, 0.0965, 0.1133,\n",
       "          0.0907, 0.1113], device='cuda:0', requires_grad=True)),\n",
       " ('decoder.fc4.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0620,  0.0345,  0.0236,  ..., -0.2067, -0.0416,  0.0399],\n",
       "          [-0.0381, -0.1521,  0.0307,  ..., -0.0742, -0.1227,  0.0800],\n",
       "          [-0.1586,  0.0708, -0.0798,  ...,  0.1179,  0.0285, -0.1847],\n",
       "          ...,\n",
       "          [ 0.0032,  0.1217, -0.1420,  ...,  0.0881, -0.1176, -0.0460],\n",
       "          [-0.1200, -0.0438,  0.0464,  ..., -0.0284, -0.0491, -0.1948],\n",
       "          [-0.1787,  0.1536, -0.2829,  ..., -0.0883,  0.0768, -0.0255]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('decoder.fc4.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.0882, 0.0573, 0.0598,  ..., 0.0856, 0.0732, 0.0729], device='cuda:0',\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.XI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../model/'\n",
    "t = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Caricato il checkpoint all'epoca:{t} \")\n",
    "model = Autoencoder(2601,2).to(device)\n",
    "optimizer = model.configure_optimizers()\n",
    "\n",
    "f1 = path + 'model' + '_' + str(t) + 'epochs' + '.pt'\n",
    "\n",
    "checkpoint = torch.load(f1)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(loss_list)\n",
    "data.to_csv('./data'+'_'+str(t)+'epochs'+'.csv')\n",
    "for key in loss_list.keys():\n",
    "    plt.plot(loss_list[key],label = key)\n",
    "plt.plot(loss_val_list,label = 'validation')    \n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Loss con batch size di '+str(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./data'+'_'+str(t)+'epochs'+'.csv') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
