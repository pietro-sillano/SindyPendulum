{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A pendulum video and autoencoder\n","metadata":{"id":"evH6sjsxsKWE"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport numpy as np\nimport sys\nsyspath = 'SindyPendulum/'\nif syspath not in sys.path:\n    sys.path.append(syspath)","metadata":{"id":"trlxwPDloKs5","execution":{"iopub.status.busy":"2022-01-03T00:16:12.028950Z","iopub.execute_input":"2022-01-03T00:16:12.029200Z","iopub.status.idle":"2022-01-03T00:16:13.415234Z","shell.execute_reply.started":"2022-01-03T00:16:12.029172Z","shell.execute_reply":"2022-01-03T00:16:13.414456Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))","metadata":{"id":"qNbEXw0Uo3zR","outputId":"7eabf512-f2cf-4429-bc0e-9af2374ed4cd","execution":{"iopub.status.busy":"2022-01-03T00:16:13.625876Z","iopub.execute_input":"2022-01-03T00:16:13.626258Z","iopub.status.idle":"2022-01-03T00:16:13.677908Z","shell.execute_reply.started":"2022-01-03T00:16:13.626223Z","shell.execute_reply":"2022-01-03T00:16:13.677188Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Generation","metadata":{"id":"8VneYLUfl5bz"}},{"cell_type":"code","source":"!git clone https://github.com/pietro-sillano/SindyPendulum.git","metadata":{"id":"ojotcMcmDoRF","outputId":"f8784ec7-cc45-4978-9da9-c63afc4a8be0","execution":{"iopub.status.busy":"2022-01-03T00:16:16.104033Z","iopub.execute_input":"2022-01-03T00:16:16.104754Z","iopub.status.idle":"2022-01-03T00:16:20.524345Z","shell.execute_reply.started":"2022-01-03T00:16:16.104700Z","shell.execute_reply":"2022-01-03T00:16:20.523227Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!cd SindyPendulum && git pull","metadata":{"id":"jiqk7akK2aoa","outputId":"fc81c54e-7390-4c2c-92f9-8d69fe224a7f","execution":{"iopub.status.busy":"2022-01-03T00:03:52.958384Z","iopub.execute_input":"2022-01-03T00:03:52.958683Z","iopub.status.idle":"2022-01-03T00:03:54.063946Z","shell.execute_reply.started":"2022-01-03T00:03:52.958645Z","shell.execute_reply":"2022-01-03T00:03:54.063047Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sindy_library import SINDyLibrary","metadata":{"id":"wwWPtC6xKbKb","execution":{"iopub.status.busy":"2022-01-03T00:16:20.526176Z","iopub.execute_input":"2022-01-03T00:16:20.526453Z","iopub.status.idle":"2022-01-03T00:16:20.534839Z","shell.execute_reply.started":"2022-01-03T00:16:20.526420Z","shell.execute_reply":"2022-01-03T00:16:20.533405Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!python SindyPendulum/data_pendulum_image.py  -i 60","metadata":{"id":"d1DKMT2rsJPt","outputId":"572f26bd-2cd1-47e4-8ebf-c37d6add998a","execution":{"iopub.status.busy":"2022-01-02T23:58:13.983059Z","iopub.execute_input":"2022-01-02T23:58:13.983627Z","iopub.status.idle":"2022-01-02T23:59:26.659515Z","shell.execute_reply.started":"2022-01-02T23:58:13.983591Z","shell.execute_reply":"2022-01-02T23:59:26.658652Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os\nimport argparse, sys\nimport numpy as np\nfrom scipy.integrate import odeint\n\ndef pend(y, t,):\n    theta, omega = y\n    dydt = [omega, - np.sin(theta)]\n    return dydt\n\ndef pend_damp(y, t,):\n    theta, omega = y\n    dydt = [omega, - np.sin(theta)-0.1*omega]\n    return dydt\n\ndef select_ics(theta0,omegaics0):\n    ics = []\n    for i in range(n_ics):\n        for j in range(n_ics):\n            lim = (np.abs((omega0[j]**2)/2 - np.cos(theta0[i])))\n            if lim <  0.99 :\n                ics.append((theta0[i],omega0[j]))\n    return ics\n\ndef image_gen(ics):\n    x = np.linspace(-1.5, 1.5, NX)\n    y = np.linspace(-1.5, 1.5, NY)\n    xx,yy = np.meshgrid(x, y)\n\n\n    data = np.empty([len(ics), len(t), len(x), len(y)],dtype = np.float32)\n    data2 = np.empty([len(ics), len(t), len(x), len(y)],dtype = np.float32)\n\n\n    for idx in range(len(ics)):\n        if(idx%100==0): print(idx,' su ', len(ics))\n        y0 = [ics[idx][0], ics[idx][1]]\n        sol = odeint(pend, y0,t)\n        theta = sol[:,0]\n        omega = sol[:,1]\n\n        temp = []\n        for i in range(len(theta)):\n            z = np.exp(- 20 *((xx - np.cos(theta[i] + np.pi/2))*(xx - \n                np.cos(theta[i] +np.pi/2))) - 20 * ((yy -np.sin(theta[i]+np.pi/2))*(yy -np.sin(theta[i]+np.pi/2))))\n            z = ((z - np.min(z))/(np.max(z)-np.min(z)))\n\n            temp.append(z)\n        data[idx] = np.array(temp)\n        \n        temp = []\n        for i in range(len(omega)):\n            exp = np.exp(- 20 *((xx - np.cos(omega[i] + np.pi/2))*(xx - \n                np.cos(omega[i] +np.pi/2))) - 20 * ((yy -np.sin(omega[i]+np.pi/2))*(yy -np.sin(omega[i]+np.pi/2))))\n\n            z = -20*(2*(xx - np.cos(theta[i]-np.pi/2))*np.sin(theta[i]-np.pi/2)*omega[i] \n                        + 2*(yy - np.sin(theta[i]-np.pi/2))*(-np.cos(theta[i]-np.pi/2))*omega[i])\n            z = z*exp\n            z = ((z - np.min(z))/(np.max(z)-np.min(z)))\n\n            temp.append(z)\n        data2[idx] = np.array(temp)\n        \n    return data,data2","metadata":{"execution":{"iopub.status.busy":"2022-01-03T00:16:22.232467Z","iopub.execute_input":"2022-01-03T00:16:22.233007Z","iopub.status.idle":"2022-01-03T00:16:22.693572Z","shell.execute_reply.started":"2022-01-03T00:16:22.232966Z","shell.execute_reply":"2022-01-03T00:16:22.692822Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"n_ics=60\n    \n#COSTANTI e PARAMETRI\nta = 0.\ntb = 5.\ndt = 0.05  \n# tb = 10\n# dt = 0.1\nNX = 51\nNY = 51\n\n\nt = np.arange(ta, tb ,dt)\ntheta0 = np.linspace(-np.pi,np.pi,n_ics)\nomega0 = np.linspace(-2.1, 2.1,n_ics)\n\nics = select_ics(theta0,omega0)\ndata,data2 = image_gen(ics)\n\n\n#questo reshape serve per mandare al autoencoder delle immagini flat\n#TODO verifica che sia corretto questo rehsape --> dovrebb essere ok fatto prova su colab\ndata = data.reshape((len(ics) * len(t),NX * NY))\ndata2 = data2.reshape((len(ics) * len(t),NX * NY))\n\nX = data\nXdot = data2\n","metadata":{"execution":{"iopub.status.busy":"2022-01-03T00:16:33.743434Z","iopub.execute_input":"2022-01-03T00:16:33.743701Z","iopub.status.idle":"2022-01-03T00:17:21.117106Z","shell.execute_reply.started":"2022-01-03T00:16:33.743671Z","shell.execute_reply":"2022-01-03T00:17:21.116387Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#X = np.load('./X.npy')\n#Xdot = np.load('./Xdot.npy')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T00:17:21.118880Z","iopub.execute_input":"2022-01-03T00:17:21.119179Z","iopub.status.idle":"2022-01-03T00:17:21.123257Z","shell.execute_reply.started":"2022-01-03T00:17:21.119137Z","shell.execute_reply":"2022-01-03T00:17:21.122469Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X = torch.from_numpy(X).float().to(device)\nXdot = torch.from_numpy(Xdot).float().to(device)\nX.shape, Xdot.shape, X.dtype","metadata":{"id":"UW2N4TZTbYsO","execution":{"iopub.status.busy":"2022-01-03T00:17:21.124377Z","iopub.execute_input":"2022-01-03T00:17:21.125165Z","iopub.status.idle":"2022-01-03T00:17:24.669274Z","shell.execute_reply.started":"2022-01-03T00:17:21.125121Z","shell.execute_reply":"2022-01-03T00:17:24.668513Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"val_size = round(X.shape[0] * 0.1)\ntrain_size = X.shape[0] - val_size\ntrain_size, val_size","metadata":{"execution":{"iopub.status.busy":"2022-01-03T00:17:24.671373Z","iopub.execute_input":"2022-01-03T00:17:24.671707Z","iopub.status.idle":"2022-01-03T00:17:24.677816Z","shell.execute_reply.started":"2022-01-03T00:17:24.671670Z","shell.execute_reply":"2022-01-03T00:17:24.677099Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"batch_size = 1024\nmy_dataset = TensorDataset(X,Xdot)\n#dataloader = DataLoader(my_dataset,batch_size=batch_size, shuffle=True)","metadata":{"id":"B2vYJ447_sxQ","execution":{"iopub.status.busy":"2022-01-03T00:17:24.679137Z","iopub.execute_input":"2022-01-03T00:17:24.679543Z","iopub.status.idle":"2022-01-03T00:17:24.687731Z","shell.execute_reply.started":"2022-01-03T00:17:24.679506Z","shell.execute_reply":"2022-01-03T00:17:24.687051Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_subset, val_subset = torch.utils.data.random_split(my_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_subset, shuffle=True, batch_size=batch_size)\nval_loader   = DataLoader(val_subset, shuffle=False, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T00:17:24.688827Z","iopub.execute_input":"2022-01-03T00:17:24.689154Z","iopub.status.idle":"2022-01-03T00:17:24.719362Z","shell.execute_reply.started":"2022-01-03T00:17:24.689118Z","shell.execute_reply":"2022-01-03T00:17:24.718763Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Network\n\n\n","metadata":{"id":"9lRG3Ea4CWWl"}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size,latent_dim):\n        super(Encoder, self).__init__()\n        self.fc1 = nn.Linear(input_size,128)\n        self.fc2 = nn.Linear(128,64)\n        self.fc3 = nn.Linear(64,32)\n        self.fc4 = nn.Linear(32,latent_dim)\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        for m in self.modules():\n            print(m)\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))   \n        x = torch.relu(self.fc4(x))\n        return x \n\nclass Decoder(nn.Module):\n    def __init__(self, input_size,latent_dim):\n        super(Decoder, self).__init__()\n        self.fc1 = nn.Linear(latent_dim,32)\n        self.fc2 = nn.Linear(32,64)\n        self.fc3 = nn.Linear(64,128)\n        self.fc4 = nn.Linear(128,input_size)\n        self.initialize_weights()\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))   \n        x = torch.relu(self.fc4(x))\n        return x\n\n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_size, latent_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = Encoder(input_size,latent_dim)\n        self.decoder = Decoder(input_size,latent_dim)\n        self.SINDyLibrary = SINDyLibrary(\n            device=device,\n            latent_dim=latent_dim,\n            include_biases=False,\n            include_states=True,\n            include_sin=True,\n            include_cos=False,\n            include_multiply_pairs=False,\n            poly_order=1,\n            include_sqrt=False,\n            include_inverse=False,\n            include_sign_sqrt_of_diff=False)\n        \n\n        self.XI = nn.Parameter(torch.full((self.SINDyLibrary.number_candidate_functions,latent_dim),1,dtype = torch.float32,requires_grad=True,device = device))\n\n        self.XI_coefficient_mask = torch.ones((self.SINDyLibrary.number_candidate_functions,latent_dim),dtype = torch.float32, device=device)\n\n    def configure_optimizers(self):\n        learning_rate = 1e-4\n #       return torch.optim.SGD(self.parameters(), lr=0.1)\n        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n\n\n    def t_derivative(self,input, xdot, weights, biases, activation='sigmoid'):\n        \"\"\"\n        Compute the first order time derivatives by propagating through the network.\n        da[l]dt = xdot * da[l]dx = xdot * product(g'(w[l]a[l-1] + b[l])* w[l])\n        Arguments:\n            input - 2D tensorflow array, input to the network. Dimensions are number of time points\n            by number of state variables.\n            xdot - First order time derivatives of the input to the network. quello che conosciamo\n            weights - List of tensorflow arrays containing the network weights\n            biases - List of tensorflow arrays containing the network biases\n            activation - String specifying which activation function to use. Options are\n            'elu' (exponential linear unit), 'relu' (rectified linear unit), 'sigmoid',\n            or linear.\n\n        Returns:\n            dadt - Tensorflow array, first order time derivatives of the network output.\n        \"\"\"\n        a   = input\n        dadt = xdot #per le condizioni iniziali\n\n        if activation == 'sigmoid':\n            for i in range(len(weights) - 1):\n                z = torch.matmul(a, weights[i].T) + biases[i]\n                a = torch.sigmoid(z)\n                gprime = a * (1-a)\n                dadt = gprime * torch.matmul(dadt, weights[i].T)\n            dadt = torch.matmul(dadt, weights[-1].T) #fuori dal ciclo bisogna ancora moltiplicare per i pesi dell ultimo livello\n            \n        elif activation == 'relu':\n            for i in range(len(weights) - 1):\n                z = torch.matmul(a, weights[i].T) + biases[i]\n                a = torch.relu(z)\n                dadt = (z > 0).float() * torch.matmul(dadt, weights[i].T)    \n            dadt = torch.matmul(dadt, weights[-1].T) #fuori dal ciclo bisogna ancora moltiplicare per i pesi dell ultimo livello\n        return dadt #nel caso che ci serve dadt sará l output dell encoder ossia le latent variables!\n\n    \n    \n    def compute_quantities(self,x,xdot):\n    \n        z = self.encoder(x)\n        xtilde = self.decoder(z)\n\n        theta = self.SINDyLibrary.transform(z) \n        zdot_hat = torch.matmul(theta, self.XI_coefficient_mask * self.XI)\n        \n        encoder_parameters = list(self.encoder.parameters())\n        encoder_weight_list = [w for w in encoder_parameters if len(w.shape) == 2]\n        encoder_biases_list = [b for b in encoder_parameters if len(b.shape) == 1]\n        zdot = self.t_derivative(x, xdot, encoder_weight_list, encoder_biases_list, activation='relu')                                               \n\n        #print(\"propagazione sul decoder\")\n        decoder_parameters = list(self.decoder.parameters())\n        decoder_weight_list = [w for w in decoder_parameters if len(w.shape) == 2]\n        decoder_biases_list = [b for b in decoder_parameters if len(b.shape) == 1]\n        xtildedot = self.t_derivative(z, zdot_hat, decoder_weight_list, decoder_biases_list, activation='relu')    \n        \n        if(DEBUG == True):\n            print('z',z)\n            print(' xtilde',xtilde)\n            print('theta',theta)\n            print('max theta',theta.max())\n            print('zdot',zdot)\n            print('zdothat',zdot_hat)\n            print('xtildedot',xtildedot)\n        return xtilde, xtildedot, z, zdot, zdot_hat\n\n    def loss_function(self, x, xdot, xtilde, xtildedot, zdot, zdot_hat,XI):\n        mse = nn.MSELoss()\n        alpha1 = 5e-4\n        alpha2 = 5e-5\n        alpha3 = 1e-5\n        #alpha1 = 5e-3\n        #alpha2 = 5e-4\n        #alpha3 = 1e-4\n        loss = {}\n        loss['recon_loss'] = mse(x, xtilde) #errore di ricostruzione \n        loss ['sindy_loss_x'] = mse(xdot, xtildedot) \n        loss ['sindy_loss_z'] = mse(zdot, zdot_hat) \n        loss['sindy_regular_loss'] = torch.sum(torch.abs(XI)) #norma L1 degli XI\n        loss['tot'] = loss['recon_loss'] + alpha1*loss['sindy_loss_x'] + alpha2*loss['sindy_loss_z'] + alpha3*loss['sindy_regular_loss']\n        tot = loss['tot']\n        return tot, loss\n    \n    def forward(self, x, xdot):\n        return self.compute_quantities(x, xdot)","metadata":{"id":"5ZSdKOTO8dFN","execution":{"iopub.status.busy":"2022-01-03T00:17:24.720511Z","iopub.execute_input":"2022-01-03T00:17:24.721165Z","iopub.status.idle":"2022-01-03T00:17:24.755664Z","shell.execute_reply.started":"2022-01-03T00:17:24.721128Z","shell.execute_reply":"2022-01-03T00:17:24.754817Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"input_size = X.shape[1]\nlatent_dim = 2","metadata":{"id":"c4mBzZuzxPYP","execution":{"iopub.status.busy":"2022-01-03T00:17:24.756902Z","iopub.execute_input":"2022-01-03T00:17:24.757394Z","iopub.status.idle":"2022-01-03T00:17:24.767963Z","shell.execute_reply.started":"2022-01-03T00:17:24.757356Z","shell.execute_reply":"2022-01-03T00:17:24.767120Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = Autoencoder(input_size,latent_dim).to(device)","metadata":{"id":"sOuh7zJeoRmH","execution":{"iopub.status.busy":"2022-01-03T00:17:24.769257Z","iopub.execute_input":"2022-01-03T00:17:24.769719Z","iopub.status.idle":"2022-01-03T00:17:24.813672Z","shell.execute_reply.started":"2022-01-03T00:17:24.769685Z","shell.execute_reply":"2022-01-03T00:17:24.813005Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model.SINDyLibrary.number_candidate_functions","metadata":{"id":"8r_OGkXz2Mgl","execution":{"iopub.status.busy":"2022-01-03T00:17:24.816035Z","iopub.execute_input":"2022-01-03T00:17:24.816264Z","iopub.status.idle":"2022-01-03T00:17:24.821869Z","shell.execute_reply.started":"2022-01-03T00:17:24.816232Z","shell.execute_reply":"2022-01-03T00:17:24.821032Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Parameters and Training\n","metadata":{"id":"6ZqwZy2XmMum"}},{"cell_type":"code","source":"def sequential_threshold(t):\n    if (t % 50 == 0 and t>1):\n        model.XI_coefficient_mask = torch.abs(model.XI) > 0.01\n        print_model()","metadata":{"id":"zrBLCBxXTPaa","execution":{"iopub.status.busy":"2022-01-03T00:17:24.823464Z","iopub.execute_input":"2022-01-03T00:17:24.823855Z","iopub.status.idle":"2022-01-03T00:17:24.829959Z","shell.execute_reply.started":"2022-01-03T00:17:24.823816Z","shell.execute_reply":"2022-01-03T00:17:24.829190Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def print_model():\n    coefficient_mask = model.XI_coefficient_mask.cpu().detach().numpy()\n    XI = model.XI.cpu().detach().numpy()\n    feature_list = model.SINDyLibrary.get_feature_names()\n    for j in range(latent_dim):\n        for i in range(len(feature_list)-1):\n            coeff = XI[i][j] * coefficient_mask[i][j]\n            if  coeff !=0:\n                print(f\"dz{j} = {coeff:.4f} {feature_list[i]}\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T00:17:24.831317Z","iopub.execute_input":"2022-01-03T00:17:24.832197Z","iopub.status.idle":"2022-01-03T00:17:24.839558Z","shell.execute_reply.started":"2022-01-03T00:17:24.832158Z","shell.execute_reply":"2022-01-03T00:17:24.838860Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"epochs = 2000\nDEBUG = False\nVALIDATION = False\nSAVE = False\n\n\nloss_list = {}\nloss_list['recon_loss'] = []\nloss_list['sindy_loss_x'] = []\nloss_list['sindy_loss_z'] = []\nloss_list['sindy_regular_loss'] = []\n#loss_list['validation'] = []\nloss_list['tot'] = []\nloss_val_list = []\n\n\nfor t in range(epochs):\n    #start = time.time()\n    \n    #queste serve per le loss di ogni batch\n    loss_epoch = {}\n    loss_epoch['recon_loss'] = []\n    loss_epoch['sindy_loss_x'] = []\n    loss_epoch['sindy_loss_z'] = []\n    loss_epoch['sindy_regular_loss'] = []\n    #loss_epoch['validation'] = []\n    loss_epoch['tot'] = []\n    loss_val_epoch = []\n    \n    model.train()\n    for batch, (X,Xdot) in enumerate (train_loader):\n        X.to(device) #per passarlo alla gpu\n        Xdot.to(device)\n        #forward pass \n        xtilde, xtildedot, z, zdot, zdot_hat = model(X,Xdot)\n\n        # Backpropagation\n        optimizer = model.configure_optimizers()\n        loss, loss_dict = model.loss_function(X, Xdot, xtilde, xtildedot, zdot, zdot_hat,model.XI)\n        loss.backward()\n        del  xtilde, xtildedot, z, zdot, zdot_hat,loss\n        optimizer.step()\n        optimizer.zero_grad()\n\n        current = batch * len(X)\n\n        for key in loss_epoch.keys():\n            loss_epoch[key].append(loss_dict[key].item())\n        del loss_dict\n\n    for key in loss_epoch.keys():\n        loss_list[key].append(sum(loss_epoch[key])/len(loss_epoch[key]))\n    del loss_epoch\n    \n    \n    #validation step\n    if VALIDATION == True:\n        model.eval()     # Optional when not using Model Specific layer\n        for batch, (X,Xdot) in enumerate (val_loader):\n            X.to(device) #per passarlo alla gpu\n            Xdot.to(device)\n            #forward pass \n            xtilde, xtildedot, z, zdot, zdot_hat = model(X,Xdot)\n            # validation loss \n            loss, _ = model.loss_function(X, Xdot, xtilde, xtildedot, zdot, zdot_hat,model.XI)\n            loss_val_epoch.append(loss.item())\n            del  xtilde, xtildedot, z, zdot, zdot_hat,loss\n        \n        loss_val_list.append(sum(loss_val_epoch)/len(loss_val_epoch))\n\n    #end = time.time()\n    if t % 20 == 0:\n        print()\n        print(f\"Epoch {t}\\n-------------------------------\")\n        for key in loss_list.keys():\n            temp = loss_list[key]\n            print(f'{key} of epoch {t}: {temp[-1]:.3e}')\n        #print(f'val loss of epoch {t}: {loss_val_list[-1]:.3e}')\n        #print('time: ',end-start)\n        \n    if SAVE == True:\n        if t % 100 ==0 and t>1 and loss_val_list[t] < loss_val_list[t-1]:\n            f1 = './model'+str(batch_size)+'_'+str(t)+'epochs'+'.pt'\n            torch.save(model,f1)\n            print('salvato il',f1)\n\nprint(\"Done!\")\ntorch.save(model,'./model.pt')","metadata":{"id":"1vtgRajRvOZK","execution":{"iopub.status.busy":"2022-01-03T00:17:24.842475Z","iopub.execute_input":"2022-01-03T00:17:24.842879Z","iopub.status.idle":"2022-01-03T00:20:03.243032Z","shell.execute_reply.started":"2022-01-03T00:17:24.842852Z","shell.execute_reply":"2022-01-03T00:20:03.241823Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"coefficient_mask = model.XI_coefficient_mask.cpu().detach().numpy()\nXI = model.XI.cpu().detach().numpy()\nfeature_list = model.SINDyLibrary.get_feature_names()\n\nprint_model()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T00:20:08.465278Z","iopub.execute_input":"2022-01-03T00:20:08.465522Z","iopub.status.idle":"2022-01-03T00:20:08.475285Z","shell.execute_reply.started":"2022-01-03T00:20:08.465495Z","shell.execute_reply":"2022-01-03T00:20:08.474452Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndata = pd.DataFrame.from_dict(loss_list)\ndata.to_csv('./data'+str(batch_size)+'_'+str(t)+'epochs'+'.csv')\ntorch.save(model,'./model'+str(batch_size)+'.pt')\nfor key in loss_list.keys():\n    plt.plot(loss_list[key],label = key)\nplt.plot(loss_val_list,label = 'validation')    \nplt.legend(loc = 'best')\nplt.title('Loss con batch size di '+str(batch_size))","metadata":{"id":"EK0Knpr4BEb-","outputId":"d14b5ec0-8037-4838-e503-99dee6d981eb","execution":{"iopub.status.busy":"2021-12-30T22:20:47.425243Z","iopub.execute_input":"2021-12-30T22:20:47.425904Z","iopub.status.idle":"2021-12-30T22:20:47.785727Z","shell.execute_reply.started":"2021-12-30T22:20:47.425867Z","shell.execute_reply":"2021-12-30T22:20:47.785067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.semilogy(loss_val_list)\nplt.loglog(loss_list['tot'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}