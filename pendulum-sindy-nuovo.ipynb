{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evH6sjsxsKWE"
   },
   "source": [
    "# A pendulum video and autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "trlxwPDloKs5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import sys\n",
    "syspath = 'SindyPendulum/'\n",
    "if syspath not in sys.path:\n",
    "    sys.path.append(syspath)\n",
    "    \n",
    "from sindy_library import SINDyLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qNbEXw0Uo3zR",
    "outputId": "7eabf512-f2cf-4429-bc0e-9af2374ed4cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VneYLUfl5bz"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pend(y, t,):\n",
    "    theta, omega = y\n",
    "    dydt = [omega, - np.sin(theta)]\n",
    "    return dydt\n",
    "\n",
    "def pend_damp(y, t,):\n",
    "    theta, omega = y\n",
    "    dydt = [omega, - np.sin(theta)-0.1*omega]\n",
    "    return dydt\n",
    "\n",
    "def select_ics(theta0,omegaics0):\n",
    "    ics = []\n",
    "    for i in range(n_ics):\n",
    "        for j in range(n_ics):\n",
    "            lim = (np.abs((omega0[j]**2)/2 - np.cos(theta0[i])))\n",
    "            if lim <  0.99 :\n",
    "                ics.append((theta0[i],omega0[j]))\n",
    "    return ics\n",
    "\n",
    "def wrap_to_pi(z):\n",
    "    z_mod = z % (2*np.pi)\n",
    "    subtract_m = (z_mod > np.pi) * (-2*np.pi)\n",
    "    return z_mod + subtract_m\n",
    "\n",
    "def image_gen(ics):\n",
    "    x = np.linspace(-1.5, 1.5, NX)\n",
    "    y = np.linspace(-1.5, 1.5, NY)\n",
    "    xx,yy = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "    data = np.empty([len(ics), len(t), len(x), len(y)],dtype = np.float32)\n",
    "    data2 = np.empty([len(ics), len(t), len(x), len(y)],dtype = np.float32)\n",
    "\n",
    "\n",
    "    for idx in range(len(ics)):\n",
    "        if(idx%100==0): print(idx,' su ', len(ics))\n",
    "        y0 = [ics[idx][0], ics[idx][1]]\n",
    "        sol = odeint(pend, y0,t)\n",
    "        theta = sol[:,0]\n",
    "        omega = sol[:,1]\n",
    "\n",
    "        temp = []\n",
    "        for i in range(len(theta)):\n",
    "            z = np.exp(- 20 *((xx - np.cos(theta[i] + np.pi/2))*(xx - \n",
    "                np.cos(theta[i] +np.pi/2))) - 20 * ((yy -np.sin(theta[i]+np.pi/2))*(yy -np.sin(theta[i]+np.pi/2))))\n",
    "            z = ((z - np.min(z))/(np.max(z)-np.min(z)))\n",
    "    \n",
    "            temp.append(z)\n",
    "        data[idx] = np.array(temp)\n",
    "        \n",
    "        temp = []\n",
    "        for i in range(len(omega)):            \n",
    "            exp = -20 * 2 * omega[i]*(np.cos(theta[i]+ np.pi/2) - np.sin(theta[i] + np.pi/2))\n",
    "            z = np.exp(- 20 *((xx - np.cos(theta[i] + np.pi/2))*(xx - \n",
    "                np.cos(theta[i] +np.pi/2))) - 20 * ((yy -np.sin(theta[i]+np.pi/2))*(yy -np.sin(theta[i]+np.pi/2))))\n",
    "            z = z * exp\n",
    "            z = ((z - np.min(z))/(np.max(z)-np.min(z)))\n",
    "            temp.append(z)\n",
    "        data2[idx] = np.array(temp)\n",
    "        \n",
    "    return data,data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  su  3724\n",
      "100  su  3724\n",
      "200  su  3724\n",
      "300  su  3724\n",
      "400  su  3724\n",
      "500  su  3724\n",
      "600  su  3724\n",
      "700  su  3724\n",
      "800  su  3724\n",
      "900  su  3724\n",
      "1000  su  3724\n",
      "1100  su  3724\n",
      "1200  su  3724\n",
      "1300  su  3724\n",
      "1400  su  3724\n",
      "1500  su  3724\n",
      "1600  su  3724\n",
      "1700  su  3724\n",
      "1800  su  3724\n",
      "1900  su  3724\n",
      "2000  su  3724\n",
      "2100  su  3724\n",
      "2200  su  3724\n",
      "2300  su  3724\n",
      "2400  su  3724\n",
      "2500  su  3724\n",
      "2600  su  3724\n",
      "2700  su  3724\n",
      "2800  su  3724\n",
      "2900  su  3724\n",
      "3000  su  3724\n",
      "3100  su  3724\n",
      "3200  su  3724\n",
      "3300  su  3724\n",
      "3400  su  3724\n",
      "3500  su  3724\n",
      "3600  su  3724\n",
      "3700  su  3724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(372400, 2601)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ics = 80\n",
    "    \n",
    "#COSTANTI e PARAMETRI\n",
    "ta = 0.\n",
    "tb = 5\n",
    "dt = 0.05\n",
    "# tb = 10\n",
    "# dt = 0.1\n",
    "NX = 51\n",
    "NY = 51\n",
    "\n",
    "\n",
    "t = np.arange(ta, tb ,dt)\n",
    "theta0 = np.linspace(-np.pi,np.pi,n_ics)\n",
    "omega0 = np.linspace(-2.1, 2.1,n_ics)\n",
    "\n",
    "ics = select_ics(theta0,omega0)\n",
    "data,data2 = image_gen(ics)\n",
    "\n",
    "\n",
    "#questo reshape serve per mandare al autoencoder delle immagini flat\n",
    "#TODO verifica che sia corretto questo rehsape --> dovrebb essere ok fatto prova su colab\n",
    "X = data.reshape((len(ics) * len(t),NX * NY))\n",
    "Xdot = data2.reshape((len(ics) * len(t),NX * NY))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fb14e229eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[123].reshape((51,51)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-687d14e2d4fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2907\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2909\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   2910\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5607\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5609\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5610\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Needed e.g. to apply png palette.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_masked_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         if (self._A.dtype != np.uint8 and\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_masked_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# If we have already made a copy, do the byteswap in place, else make a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xdot[123].reshape((51,51)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdot[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UW2N4TZTbYsO",
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bb0f087e87bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mXdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "X = torch.from_numpy(X).float().to(device)\n",
    "Xdot = torch.from_numpy(Xdot).float().to(device)\n",
    "X.shape, Xdot.shape, X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335160, 37240)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = round(X.shape[0] * 0.1)\n",
    "train_size = X.shape[0] - val_size\n",
    "train_size, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "B2vYJ447_sxQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "my_dataset = TensorDataset(X,Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_subset, val_subset = torch.utils.data.random_split(my_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, shuffle=True, batch_size=batch_size)\n",
    "val_loader   = DataLoader(val_subset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lRG3Ea4CWWl"
   },
   "source": [
    "# Network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5ZSdKOTO8dFN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size,latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,latent_dim)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            print(m)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #nn.init.xavier_normal_(m.weight) #per la sigmoid\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))   \n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size,latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,128)\n",
    "        self.fc4 = nn.Linear(128,input_size)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))   \n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_size,latent_dim)\n",
    "        self.decoder = Decoder(input_size,latent_dim)\n",
    "        self.SINDyLibrary = SINDyLibrary(\n",
    "            device=device,\n",
    "            latent_dim=latent_dim,\n",
    "            include_biases=True,\n",
    "            include_states=True,\n",
    "            include_sin=True,\n",
    "            include_cos=True,\n",
    "            include_multiply_pairs=False, #non ho capito cosa é ma conta due volte le coppie  \n",
    "            poly_order=2,\n",
    "            include_sqrt=False,\n",
    "            include_inverse=False,\n",
    "            include_sign_sqrt_of_diff=False)\n",
    "        \n",
    "\n",
    "#        self.XI = nn.Parameter(torch.full((self.SINDyLibrary.number_candidate_functions,latent_dim),1.,dtype = torch.float32,requires_grad=True,device = device))\n",
    "        self.XI = nn.Parameter(torch.randn((self.SINDyLibrary.number_candidate_functions,latent_dim),\n",
    "                                                   dtype = torch.float32,requires_grad=True,device = device))\n",
    "\n",
    "        self.XI_coefficient_mask = torch.ones((self.SINDyLibrary.number_candidate_functions,latent_dim),dtype = torch.float32, device=device)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        learning_rate = 1e-4\n",
    "        #return torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def t_derivative(self,input, xdot, weights, biases, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Compute the first order time derivatives by propagating through the network.\n",
    "        da[l]dt = xdot * da[l]dx = xdot * product(g'(w[l]a[l-1] + b[l])* w[l])\n",
    "        Arguments:\n",
    "            input - 2D tensorflow array, input to the network. Dimensions are number of time points\n",
    "            by number of state variables.\n",
    "            xdot - First order time derivatives of the input to the network. quello che conosciamo\n",
    "            weights - List of tensorflow arrays containing the network weights\n",
    "            biases - List of tensorflow arrays containing the network biases\n",
    "            activation - String specifying which activation function to use. Options are\n",
    "            'elu' (exponential linear unit), 'relu' (rectified linear unit), 'sigmoid',\n",
    "            or linear.\n",
    "\n",
    "        Returns:\n",
    "            dadt - Tensorflow array, first order time derivatives of the network output.\n",
    "        \"\"\"\n",
    "        a   = input\n",
    "        dadt = xdot #per le condizioni iniziali\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            for i in range(len(weights) - 1):\n",
    "                z = torch.matmul(a, weights[i].T) + biases[i]\n",
    "                a = torch.sigmoid(z)\n",
    "                gprime = a * (1-a)\n",
    "                dadt = gprime * torch.matmul(dadt, weights[i].T)\n",
    "            dadt = torch.matmul(dadt, weights[-1].T) #fuori dal ciclo bisogna ancora moltiplicare per i pesi dell ultimo livello\n",
    "            \n",
    "        elif activation == 'relu':\n",
    "            for i in range(len(weights) - 1):\n",
    "                z = torch.matmul(a, weights[i].T) + biases[i]\n",
    "                a = torch.relu(z)\n",
    "                dadt = (z > 0).float() * torch.matmul(dadt, weights[i].T)    \n",
    "            dadt = torch.matmul(dadt, weights[-1].T) #fuori dal ciclo bisogna ancora moltiplicare per i pesi dell ultimo livello\n",
    "        return dadt #nel caso che ci serve dadt sará l output dell encoder ossia le latent variables!\n",
    "\n",
    "    \n",
    "    \n",
    "    def compute_quantities(self,x,xdot):\n",
    "    \n",
    "        z = self.encoder(x)\n",
    "        xtilde = self.decoder(z)\n",
    "\n",
    "        theta = self.SINDyLibrary.transform(z) \n",
    "        zdot_hat = torch.matmul(theta, self.XI_coefficient_mask * self.XI)\n",
    "        \n",
    "        encoder_parameters = list(self.encoder.parameters())\n",
    "        encoder_weight_list = [w for w in encoder_parameters if len(w.shape) == 2]\n",
    "        encoder_biases_list = [b for b in encoder_parameters if len(b.shape) == 1]\n",
    "        zdot = self.t_derivative(x, xdot, encoder_weight_list, encoder_biases_list, activation='relu')                                               \n",
    "\n",
    "        #print(\"propagazione sul decoder\")\n",
    "        decoder_parameters = list(self.decoder.parameters())\n",
    "        decoder_weight_list = [w for w in decoder_parameters if len(w.shape) == 2]\n",
    "        decoder_biases_list = [b for b in decoder_parameters if len(b.shape) == 1]\n",
    "        xtildedot = self.t_derivative(z, zdot_hat, decoder_weight_list, decoder_biases_list, activation='relu')    \n",
    "        \n",
    "        return xtilde, xtildedot, z, zdot, zdot_hat\n",
    "\n",
    "    def loss_function(self, x, xdot, xtilde, xtildedot, zdot, zdot_hat,XI):\n",
    "        mse = nn.MSELoss()\n",
    "        alpha1 = 5e-4\n",
    "        alpha2 = 5e-5\n",
    "        alpha3 = 1e-5\n",
    "\n",
    "\n",
    "        loss = {}\n",
    "        loss['recon_loss'] = mse(x, xtilde) #errore di ricostruzione \n",
    "        loss ['sindy_loss_x'] = mse(xdot, xtildedot) \n",
    "        loss ['sindy_loss_z'] = mse(zdot, zdot_hat) \n",
    "        loss['sindy_regular_loss'] = torch.sum(torch.abs(XI)) #norma L1 degli XI\n",
    "        loss['tot'] = loss['recon_loss'] + alpha1*loss['sindy_loss_x'] + alpha2*loss['sindy_loss_z'] + alpha3*loss['sindy_regular_loss']\n",
    "        tot = loss['tot']\n",
    "        return tot, loss\n",
    "    \n",
    "    def forward(self, x, xdot):\n",
    "        return self.compute_quantities(x, xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c4mBzZuzxPYP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = X.shape[1]\n",
    "latent_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sOuh7zJeoRmH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (fc1): Linear(in_features=2601, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=2601, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(input_size,latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "8r_OGkXz2Mgl",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.SINDyLibrary.number_candidate_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.SINDyLibrary.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZqwZy2XmMum"
   },
   "source": [
    "# Parameters and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zrBLCBxXTPaa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequential_threshold(t):\n",
    "    if (t % seq_thres == 0 and t>1):\n",
    "        model.XI_coefficient_mask = torch.abs(model.XI) > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(t):\n",
    "    if SAVE == True:\n",
    "        if t % saving_rate == 0 and t > 0:\n",
    "            print('salvataggio a ',t,\"epoche\")\n",
    "            f1 = path + 'model' + '_' + str(t) + 'epochs' + '.pt'\n",
    "            torch.save({\n",
    "            'epoch': t,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': model.configure_optimizers().state_dict(),\n",
    "            'loss': model.loss_function,\n",
    "            'sindy_coefficients': model.XI,\n",
    "            'coefficient_mask' : model.XI_coefficient_mask\n",
    "            }, f1)\n",
    "            \n",
    "            XI = model.XI.cpu().detach().numpy()\n",
    "            np.save(path + 'model' + '_' + str(t) + 'epochs' + '.npy',XI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_model(t):\n",
    "    f = open(path + 'model_equation' + '_' + str(t) + '.txt', 'w')\n",
    "    coefficient_mask = model.XI_coefficient_mask.cpu().detach().numpy()\n",
    "    XI = model.XI.cpu().detach().numpy()\n",
    "    feature_list = model.SINDyLibrary.get_feature_names()\n",
    "    for j in range(latent_dim):\n",
    "        for i in range(len(feature_list)-1):\n",
    "            coeff = XI[i][j] * coefficient_mask[i][j]\n",
    "            if  abs(coeff) >= 0.1 and coeff != 0:\n",
    "                #print(f\"dz{j} = {coeff:.4f} {feature_list[i]}\")\n",
    "                f.write(f\"dz{j} = {coeff:.4f} {feature_list[i]}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training step for one epoch\n",
    "def train_step(loss_list):\n",
    "        \n",
    "    #queste serve per le loss di ogni batch\n",
    "    loss_epoch = {}\n",
    "    loss_epoch['recon_loss'] = []\n",
    "    loss_epoch['sindy_loss_x'] = []\n",
    "    loss_epoch['sindy_loss_z'] = []\n",
    "    loss_epoch['sindy_regular_loss'] = []\n",
    "    loss_epoch['tot'] = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X,Xdot) in enumerate (train_loader):\n",
    "        X.to(device) #per passarlo alla gpu\n",
    "        Xdot.to(device)\n",
    "        #forward pass \n",
    "        xtilde, xtildedot, z, zdot, zdot_hat = model(X,Xdot)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer = model.configure_optimizers()\n",
    "        loss, loss_dict = model.loss_function(X, Xdot, xtilde, xtildedot, zdot, zdot_hat,model.XI)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for key in loss_epoch.keys():\n",
    "            loss_epoch[key].append(loss_dict[key].item())\n",
    "        \n",
    "        \n",
    "    for key in loss_epoch.keys():\n",
    "            loss_list[key].append(sum(loss_epoch[key])/len(loss_epoch[key]))\n",
    "            \n",
    "    del loss_epoch,loss_dict,xtilde, xtildedot, z, zdot, zdot_hat,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(loss_val_list):\n",
    "    loss_val_epoch = []\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    for batch, (X,Xdot) in enumerate (val_loader):\n",
    "        X.to(device) #per passarlo alla gpu\n",
    "        Xdot.to(device)\n",
    "        #forward pass \n",
    "        xtilde, xtildedot, z, zdot, zdot_hat = model(X,Xdot)\n",
    "        # validation loss \n",
    "        loss, _ = model.loss_function(X, Xdot, xtilde, xtildedot, zdot, zdot_hat,model.XI)\n",
    "        loss_val_epoch.append(loss.item())\n",
    "        del  xtilde, xtildedot, z, zdot, zdot_hat,loss\n",
    "\n",
    "    loss_val_list.append(sum(loss_val_epoch)/len(loss_val_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(t):\n",
    "    if t % loss_rate == 0:\n",
    "        print()\n",
    "        print(f\"Epoch {t}\\n-------------------------------\")\n",
    "        for key in loss_list.keys():\n",
    "            temp = loss_list[key]\n",
    "            print(f'{key} of epoch {t}: {temp[-1]:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION = False\n",
    "SAVE = True\n",
    "\n",
    "epochs = 2000\n",
    "path = '../model/'\n",
    "\n",
    "seq_thres = 100   # numero epoche ogni quanto fare seq thres\n",
    "\n",
    "equation_rate = 100 #print delle equazioni trovate\n",
    "\n",
    "saving_rate = 100 # numero epoche in cui salvo il modello e gli XI\n",
    "\n",
    "loss_rate = 50  #print delle loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "recon_loss of epoch 50: 1.982e-03\n",
      "sindy_loss_x of epoch 50: 2.587e-01\n",
      "sindy_loss_z of epoch 50: 6.358e-02\n",
      "sindy_regular_loss of epoch 50: 3.622e+01\n",
      "tot of epoch 50: 2.477e-03\n",
      "epoch: 51\n",
      "epoch: 52\n",
      "epoch: 53\n",
      "epoch: 54\n",
      "epoch: 55\n",
      "epoch: 56\n",
      "epoch: 57\n",
      "epoch: 58\n",
      "epoch: 59\n",
      "epoch: 60\n",
      "epoch: 61\n",
      "epoch: 62\n",
      "epoch: 63\n",
      "epoch: 64\n",
      "epoch: 65\n",
      "epoch: 66\n",
      "epoch: 67\n",
      "epoch: 68\n",
      "epoch: 69\n",
      "epoch: 70\n",
      "epoch: 71\n",
      "epoch: 72\n",
      "epoch: 73\n",
      "epoch: 74\n",
      "epoch: 75\n",
      "epoch: 76\n",
      "epoch: 77\n",
      "epoch: 78\n",
      "epoch: 79\n",
      "epoch: 80\n",
      "epoch: 81\n",
      "epoch: 82\n",
      "epoch: 83\n",
      "epoch: 84\n",
      "epoch: 85\n",
      "epoch: 86\n",
      "epoch: 87\n",
      "epoch: 88\n",
      "epoch: 89\n",
      "epoch: 90\n",
      "epoch: 91\n",
      "epoch: 92\n",
      "epoch: 93\n",
      "epoch: 94\n",
      "epoch: 95\n",
      "epoch: 96\n",
      "epoch: 97\n",
      "epoch: 98\n",
      "epoch: 99\n",
      "epoch: 100\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "recon_loss of epoch 100: 1.971e-03\n",
      "sindy_loss_x of epoch 100: 2.492e-01\n",
      "sindy_loss_z of epoch 100: 2.981e-02\n",
      "sindy_regular_loss of epoch 100: 3.342e-01\n",
      "tot of epoch 100: 2.100e-03\n",
      "salvataggio a  100 epoche\n",
      "epoch: 101\n",
      "epoch: 102\n",
      "epoch: 103\n",
      "epoch: 104\n",
      "epoch: 105\n",
      "epoch: 106\n",
      "epoch: 107\n",
      "epoch: 108\n",
      "epoch: 109\n",
      "epoch: 110\n",
      "epoch: 111\n",
      "epoch: 112\n",
      "epoch: 113\n",
      "epoch: 114\n",
      "epoch: 115\n",
      "epoch: 116\n",
      "epoch: 117\n",
      "epoch: 118\n",
      "epoch: 119\n",
      "epoch: 120\n",
      "epoch: 121\n",
      "epoch: 122\n",
      "epoch: 123\n",
      "epoch: 124\n",
      "epoch: 125\n",
      "epoch: 126\n",
      "epoch: 127\n",
      "epoch: 128\n",
      "epoch: 129\n",
      "epoch: 130\n",
      "epoch: 131\n",
      "epoch: 132\n",
      "epoch: 133\n",
      "epoch: 134\n",
      "epoch: 135\n",
      "epoch: 136\n",
      "epoch: 137\n",
      "epoch: 138\n",
      "epoch: 139\n",
      "epoch: 140\n",
      "epoch: 141\n",
      "epoch: 142\n",
      "epoch: 143\n",
      "epoch: 144\n",
      "epoch: 145\n",
      "epoch: 146\n",
      "epoch: 147\n",
      "epoch: 148\n",
      "epoch: 149\n",
      "epoch: 150\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "recon_loss of epoch 150: 1.968e-03\n",
      "sindy_loss_x of epoch 150: 2.467e-01\n",
      "sindy_loss_z of epoch 150: 2.190e-02\n",
      "sindy_regular_loss of epoch 150: 2.384e-01\n",
      "tot of epoch 150: 2.095e-03\n",
      "epoch: 151\n",
      "epoch: 152\n",
      "epoch: 153\n",
      "epoch: 154\n",
      "epoch: 155\n",
      "epoch: 156\n",
      "epoch: 157\n",
      "epoch: 158\n",
      "epoch: 159\n",
      "epoch: 160\n",
      "epoch: 161\n",
      "epoch: 162\n",
      "epoch: 163\n",
      "epoch: 164\n",
      "epoch: 165\n",
      "epoch: 166\n",
      "epoch: 167\n",
      "epoch: 168\n",
      "epoch: 169\n",
      "epoch: 170\n",
      "epoch: 171\n",
      "epoch: 172\n",
      "epoch: 173\n",
      "epoch: 174\n",
      "epoch: 175\n",
      "epoch: 176\n",
      "epoch: 177\n",
      "epoch: 178\n",
      "epoch: 179\n",
      "epoch: 180\n",
      "epoch: 181\n",
      "epoch: 182\n",
      "epoch: 183\n",
      "epoch: 184\n",
      "epoch: 185\n",
      "epoch: 186\n",
      "epoch: 187\n",
      "epoch: 188\n",
      "epoch: 189\n",
      "epoch: 190\n",
      "epoch: 191\n",
      "epoch: 192\n",
      "epoch: 193\n",
      "epoch: 194\n",
      "epoch: 195\n",
      "epoch: 196\n",
      "epoch: 197\n",
      "epoch: 198\n",
      "epoch: 199\n",
      "epoch: 200\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "recon_loss of epoch 200: 1.967e-03\n",
      "sindy_loss_x of epoch 200: 2.457e-01\n",
      "sindy_loss_z of epoch 200: 1.863e-02\n",
      "sindy_regular_loss of epoch 200: 1.984e-01\n",
      "tot of epoch 200: 2.093e-03\n",
      "salvataggio a  200 epoche\n",
      "epoch: 201\n",
      "epoch: 202\n",
      "epoch: 203\n",
      "epoch: 204\n",
      "epoch: 205\n",
      "epoch: 206\n",
      "epoch: 207\n",
      "epoch: 208\n",
      "epoch: 209\n",
      "epoch: 210\n",
      "epoch: 211\n",
      "epoch: 212\n",
      "epoch: 213\n",
      "epoch: 214\n",
      "epoch: 215\n",
      "epoch: 216\n",
      "epoch: 217\n",
      "epoch: 218\n",
      "epoch: 219\n",
      "epoch: 220\n",
      "epoch: 221\n",
      "epoch: 222\n",
      "epoch: 223\n",
      "epoch: 224\n",
      "epoch: 225\n",
      "epoch: 226\n",
      "epoch: 227\n",
      "epoch: 228\n",
      "epoch: 229\n",
      "epoch: 230\n",
      "epoch: 231\n",
      "epoch: 232\n",
      "epoch: 233\n",
      "epoch: 234\n",
      "epoch: 235\n",
      "epoch: 236\n",
      "epoch: 237\n",
      "epoch: 238\n",
      "epoch: 239\n",
      "epoch: 240\n",
      "epoch: 241\n",
      "epoch: 242\n",
      "epoch: 243\n",
      "epoch: 244\n",
      "epoch: 245\n",
      "epoch: 246\n",
      "epoch: 247\n",
      "epoch: 248\n",
      "epoch: 249\n",
      "epoch: 250\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "recon_loss of epoch 250: 1.967e-03\n",
      "sindy_loss_x of epoch 250: 2.452e-01\n",
      "sindy_loss_z of epoch 250: 1.476e-02\n",
      "sindy_regular_loss of epoch 250: 1.760e-01\n",
      "tot of epoch 250: 2.092e-03\n",
      "epoch: 251\n",
      "epoch: 252\n",
      "epoch: 253\n",
      "epoch: 254\n",
      "epoch: 255\n",
      "epoch: 256\n",
      "epoch: 257\n",
      "epoch: 258\n",
      "epoch: 259\n",
      "epoch: 260\n",
      "epoch: 261\n",
      "epoch: 262\n",
      "epoch: 263\n",
      "epoch: 264\n",
      "epoch: 265\n",
      "epoch: 266\n",
      "epoch: 267\n",
      "epoch: 268\n",
      "epoch: 269\n",
      "epoch: 270\n",
      "epoch: 271\n",
      "epoch: 272\n",
      "epoch: 273\n",
      "epoch: 274\n",
      "epoch: 275\n",
      "epoch: 276\n",
      "epoch: 277\n",
      "epoch: 278\n",
      "epoch: 279\n",
      "epoch: 280\n",
      "epoch: 281\n",
      "epoch: 282\n",
      "epoch: 283\n",
      "epoch: 284\n",
      "epoch: 285\n",
      "epoch: 286\n",
      "epoch: 287\n",
      "epoch: 288\n",
      "epoch: 289\n",
      "epoch: 290\n",
      "epoch: 291\n",
      "epoch: 292\n",
      "epoch: 293\n",
      "epoch: 294\n",
      "epoch: 295\n",
      "epoch: 296\n",
      "epoch: 297\n",
      "epoch: 298\n",
      "epoch: 299\n",
      "epoch: 300\n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "recon_loss of epoch 300: 1.966e-03\n",
      "sindy_loss_x of epoch 300: 2.449e-01\n",
      "sindy_loss_z of epoch 300: 1.509e-02\n",
      "sindy_regular_loss of epoch 300: 1.609e-01\n",
      "tot of epoch 300: 2.091e-03\n",
      "salvataggio a  300 epoche\n",
      "epoch: 301\n",
      "epoch: 302\n",
      "epoch: 303\n",
      "epoch: 304\n",
      "epoch: 305\n",
      "epoch: 306\n",
      "epoch: 307\n",
      "epoch: 308\n",
      "epoch: 309\n",
      "epoch: 310\n",
      "epoch: 311\n",
      "epoch: 312\n",
      "epoch: 313\n",
      "epoch: 314\n",
      "epoch: 315\n",
      "epoch: 316\n",
      "epoch: 317\n",
      "epoch: 318\n",
      "epoch: 319\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-3c7fa7c00f1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-dd3d45620200>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(loss_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mXdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mxtilde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtildedot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzdot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzdot_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-6aa09ccee42b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xdot)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_quantities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-6aa09ccee42b>\u001b[0m in \u001b[0;36mcompute_quantities\u001b[0;34m(self, x, xdot)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_quantities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mxtilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-6aa09ccee42b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_list = {}\n",
    "loss_list['recon_loss'] = []\n",
    "loss_list['sindy_loss_x'] = []\n",
    "loss_list['sindy_loss_z'] = []\n",
    "loss_list['sindy_regular_loss'] = []\n",
    "loss_list['tot'] = []\n",
    "loss_val_list = []\n",
    "\n",
    "\n",
    "while t <= epochs:\n",
    "    print(\"epoch:\",t)\n",
    "    train_step(loss_list)\n",
    "    \n",
    "    validation_step(loss_val_list)\n",
    "    \n",
    "    print_loss(t)\n",
    "\n",
    "    save_model(t)\n",
    "    \n",
    "    sequential_threshold(t)\n",
    "    \n",
    "    if t % equation_rate == 0 and t>1:\n",
    "        print_model(t)\n",
    "        data = pd.DataFrame.from_dict(loss_list)\n",
    "        data.to_csv(path + 'data_'+str(t)+'epochs'+'.csv')\n",
    "    t = t + 1\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(loss_list['tot'])\n",
    "#plt.plot(loss_list['sindy_loss_z'])\n",
    "plt.plot(loss_list['sindy_loss_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../model_1e5/' # con regolarizzatore a 1e-3 non avrá senso spero\n",
    "#path = '../model_zero/' #con regolarizzatore a zero\n",
    "#path = '../model_alphae6/'  # con regolarizzatore a 1e-6\n",
    "t = 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricato il checkpoint all'epoca:1600 \n",
      "Encoder(\n",
      "  (fc1): Linear(in_features=2601, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=2601, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Caricato il checkpoint all'epoca:{t} \")\n",
    "model = Autoencoder(2601,10).to(device)\n",
    "optimizer = model.configure_optimizers()\n",
    "\n",
    "f1 = path + 'model' + '_' + str(t) + 'epochs' + '.pt'\n",
    "\n",
    "checkpoint = torch.load(f1)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(loss_list)\n",
    "data.to_csv('./data'+'_'+str(t)+'epochs'+'.csv')\n",
    "for key in loss_list.keys():\n",
    "    plt.plot(loss_list[key],label = key)\n",
    "plt.plot(loss_val_list,label = 'validation')    \n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Loss con batch size di '+str(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./data'+'_'+str(t)+'epochs'+'.csv') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
